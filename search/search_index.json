{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quinn","text":"<p>Quinn contains PySpark helper methods that will make you more productive.</p> <p>Quinn is also a great way to learn about PySpark best practices like how to organize and unit test your code.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We have a solid group of maintainers, chat on contributor meetings regularly, and eagerly accept contributions from other members.</p> <p>We want to help the world write beautiful PySpark and give them a wonderful developer experience.</p>"},{"location":"#code-style","title":"Code Style","text":"<p>We are using PySpark code-style and <code>sphinx</code> as docstrings format. For more details about <code>sphinx</code> format see this tutorial. A short example of <code>sphinx</code>-formatted docstring is placed below:</p> <pre><code>\"\"\"[Summary]\n\n:param [ParamName]: [ParamDescription], defaults to [DefaultParamVal]\n:type [ParamName]: [ParamType](, optional)\n...\n:raises [ErrorType]: [ErrorDescription]\n...\n:return: [ReturnDescription]\n:rtype: [ReturnType]\n\"\"\"\n</code></pre>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"<p>Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages and navigation.\n\nScript was taken from\nhttps://mkdocstrings.github.io/recipes/#automatic-code-reference-pages\n\"\"\"\n</pre> \"\"\"Generate the code reference pages and navigation.  Script was taken from https://mkdocstrings.github.io/recipes/#automatic-code-reference-pages \"\"\" In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>for path in sorted(Path(\".\").rglob(\"quinn/**/*.py\")):\n    module_path = path.relative_to(\".\").with_suffix(\"\")\n    doc_path = path.relative_to(\".\").with_suffix(\".md\")\n    full_doc_path = Path(\"reference\", doc_path)\n\n    parts = tuple(module_path.parts)\n\n    if parts[-1] == \"__init__\":\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n    elif parts[-1] == \"__main__\":\n        continue\n\n    nav[parts] = doc_path.as_posix()  #\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        ident = \".\".join(parts)\n        fd.write(f\"::: {ident}\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n</pre> for path in sorted(Path(\".\").rglob(\"quinn/**/*.py\")):     module_path = path.relative_to(\".\").with_suffix(\"\")     doc_path = path.relative_to(\".\").with_suffix(\".md\")     full_doc_path = Path(\"reference\", doc_path)      parts = tuple(module_path.parts)      if parts[-1] == \"__init__\":         parts = parts[:-1]         doc_path = doc_path.with_name(\"index.md\")         full_doc_path = full_doc_path.with_name(\"index.md\")     elif parts[-1] == \"__main__\":         continue      nav[parts] = doc_path.as_posix()  #      with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         ident = \".\".join(parts)         fd.write(f\"::: {ident}\")      mkdocs_gen_files.set_edit_path(full_doc_path, path) In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#quinn-helper-functions","title":"Quinn Helper Functions","text":"<pre><code>import quinn\n</code></pre>"},{"location":"usage/#dataframe-validations","title":"DataFrame Validations","text":"<p>validate_presence_of_columns()</p> <pre><code>quinn.validate_presence_of_columns(source_df, [\"name\", \"age\", \"fun\"])\n</code></pre> <p>Raises an exception unless <code>source_df</code> contains the <code>name</code>, <code>age</code>, and <code>fun</code> column.</p> <p>validate_schema()</p> <pre><code>quinn.validate_schema(source_df, required_schema)\n</code></pre> <p>Raises an exception unless <code>source_df</code> contains all the <code>StructFields</code> defined in the <code>required_schema</code>.</p> <p>validate_absence_of_columns()</p> <pre><code>quinn.validate_absence_of_columns(source_df, [\"age\", \"cool\"])\n</code></pre> <p>Raises an exception if <code>source_df</code> contains <code>age</code> or <code>cool</code> columns.</p>"},{"location":"usage/#functions","title":"Functions","text":"<p>single_space()</p> <pre><code>actual_df = source_df.withColumn(\n    \"words_single_spaced\",\n    quinn.single_space(col(\"words\"))\n)\n</code></pre> <p>Replaces all multispaces with single spaces (e.g. changes <code>\"this has   some\"</code> to <code>\"this has some\"</code>.</p> <p>remove_all_whitespace()</p> <pre><code>actual_df = source_df.withColumn(\n    \"words_without_whitespace\",\n    quinn.remove_all_whitespace(col(\"words\"))\n)\n</code></pre> <p>Removes all whitespace in a string (e.g. changes <code>\"this has some\"</code> to <code>\"thishassome\"</code>.</p> <p>anti_trim()</p> <pre><code>actual_df = source_df.withColumn(\n    \"words_anti_trimmed\",\n    quinn.anti_trim(col(\"words\"))\n)\n</code></pre> <p>Removes all inner whitespace, but doesn't delete leading or trailing whitespace (e.g. changes <code>\" this has some \"</code> to <code>\" thishassome \"</code>.</p> <p>remove_non_word_characters()</p> <pre><code>actual_df = source_df.withColumn(\n    \"words_without_nonword_chars\",\n    quinn.remove_non_word_characters(col(\"words\"))\n)\n</code></pre> <p>Removes all non-word characters from a string (e.g. changes <code>\"si%$#@!#$!@#mpsons\"</code> to <code>\"simpsons\"</code>.</p> <p>multi_equals()</p> <pre><code>source_df.withColumn(\n    \"are_s1_and_s2_cat\",\n    quinn.multi_equals(\"cat\")(col(\"s1\"), col(\"s2\"))\n)\n</code></pre> <p><code>multi_equals</code> returns true if <code>s1</code> and <code>s2</code> are both equal to <code>\"cat\"</code>.</p> <p>approx_equal()</p> <p>This function takes 3 arguments which are 2 Pyspark DataFrames and one integer values as threshold, and returns the Boolean column which tells if the columns are equal in the threshold.</p> <pre><code>let the columns be\ncol1 = [1.2, 2.5, 3.1, 4.0, 5.5]\ncol2 = [1.3, 2.3, 3.0, 3.9, 5.6]\nthreshold = 0.2\n\nresult = approx_equal(col(\"col1\"), col(\"col2\"), threshold)\nresult.show()\n\n+-----+\n|value|\n+-----+\n| true|\n|false|\n| true|\n| true|\n| true|\n+-----+\n</code></pre> <p>array_choice()</p> <p>This function takes a Column as a parameter and returns a PySpark column that contains a random value from the input column parameter</p> <pre><code>df = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [\"values\"])\nresult = df.select(array_choice(col(\"values\")))\n\nThe output is :=\n+--------------+\n|array_choice()|\n+--------------+\n|             2|\n+--------------+\n\n</code></pre> <p>regexp_extract_all()</p> <p>The regexp_extract_all takes 2 parameters String <code>s</code> and <code>regexp</code> which is a regular expression. This function finds all the matches for the string which satisfies the regular expression.</p> <pre><code>print(regexp_extract_all(\"this is a example text message for testing application\",r\"\\b\\w*a\\w*\\b\"))\n\nThe output is :=\n['a', 'example', 'message', 'application']\n\n</code></pre> <p>Where <code>r\"\\b\\w*a\\w*\\b\"</code> pattern checks for words containing letter <code>a</code></p> <p>week_start_date()</p> <p>It takes 2 parameters, column and week_start_day. It returns a Spark Dataframe column which contains the start date of the week. By default the week_start_day is set to \"Sun\".</p> <p>For input <code>[\"2023-03-05\", \"2023-03-06\", \"2023-03-07\", \"2023-03-08\"]</code> the Output is</p> <pre><code>result = df.select(\"date\", week_start_date(col(\"date\"), \"Sun\"))\nresult.show()\n+----------+----------------+\n|      date|week_start_date |\n+----------+----------------+\n|2023-03-05|      2023-03-05|\n|2023-03-07|      2023-03-05|\n|2023-03-08|      2023-03-05|\n+----------+----------------+\n</code></pre> <p>week_end_date()</p> <p>It also takes 2 Paramters as Column and week_end_day, and returns the dateframe column which contains the end date of the week. By default the week_end_day is set to \"sat\"</p> <pre><code>+---------+-------------+\n      date|week_end_date|\n+---------+-------------+\n2023-03-05|   2023-03-05|\n2023-03-07|   2023-03-12|\n2023-03-08|   2023-03-12|\n+---------+-------------+\n\n</code></pre> <p>uuid5()</p> <p>This function generates UUIDv5 in string form from the passed column and optionally namespace and optional extra salt. By default namespace is NAMESPACE_DNS UUID and no extra string used to reduce hash collisions.</p> <pre><code>\ndf = spark.createDataFrame([(\"lorem\",), (\"ipsum\",)], [\"values\"])\nresult = df.select(quinn.uuid5(F.col(\"values\")).alias(\"uuid5\"))\nresult.show(truncate=False)\n\nThe output is :=\n+------------------------------------+\n|uuid5                               |\n+------------------------------------+\n|35482fda-c10a-5076-8da2-dc7bf22d6be4|\n|51b79c1d-d06c-5b30-a5c6-1fadcd3b2103|\n+------------------------------------+\n\n</code></pre>"},{"location":"usage/#transformations","title":"Transformations","text":"<p>snake_case_col_names()</p> <pre><code>quinn.snake_case_col_names(source_df)\n</code></pre> <p>Converts all the column names in a DataFrame to snake_case. It's annoying to write SQL queries when columns aren't snake cased.</p> <p>sort_columns()</p> <pre><code>quinn.sort_columns(df=source_df, sort_order=\"asc\", sort_nested=True)\n</code></pre> <p>Sorts the DataFrame columns in alphabetical order, including nested columns if sort_nested is set to True. Wide DataFrames are easier to navigate when they're sorted alphabetically.</p>"},{"location":"usage/#dataframe-helpers","title":"DataFrame Helpers","text":"<p>column_to_list()</p> <pre><code>quinn.column_to_list(source_df, \"name\")\n</code></pre> <p>Converts a column in a DataFrame to a list of values.</p> <p>two_columns_to_dictionary()</p> <pre><code>quinn.two_columns_to_dictionary(source_df, \"name\", \"age\")\n</code></pre> <p>Converts two columns of a DataFrame into a dictionary. In this example, <code>name</code> is the key and <code>age</code> is the value.</p> <p>to_list_of_dictionaries()</p> <pre><code>quinn.to_list_of_dictionaries(source_df)\n</code></pre> <p>Converts an entire DataFrame into a list of dictionaries.</p> <p>show_output_to_df()</p> <pre><code>quinn.show_output_to_df(output_str, spark)\n</code></pre> <p>Parses a spark DataFrame output string into a spark DataFrame. Useful for quickly pulling data from a log into a DataFrame. In this example, output_str is a string of the form:</p> <pre><code>+----+---+-----------+------+\n|name|age|     stuff1|stuff2|\n+----+---+-----------+------+\n|jose|  1|nice person|  yoyo|\n|  li|  2|nice person|  yoyo|\n| liz|  3|nice person|  yoyo|\n+----+---+-----------+------+\n</code></pre>"},{"location":"usage/#schema-helpers","title":"Schema Helpers","text":"<p>schema_from_csv()</p> <pre><code>quinn.schema_from_csv(\"schema.csv\")\n</code></pre> <p>Converts a CSV file into a PySpark schema (aka <code>StructType</code>). The CSV must contain the column name and type.  The nullable and metadata columns are optional.</p> <p>Here's an example CSV file:</p> <pre><code>name,type\nperson,string\naddress,string\nphoneNumber,string\nage,int\n</code></pre> <p>Here's how to convert that CSV file to a PySpark schema:</p> <pre><code>schema = schema_from_csv(spark, \"some_file.csv\")\n\nStructType([\n    StructField(\"person\", StringType(), True),\n    StructField(\"address\", StringType(), True),\n    StructField(\"phoneNumber\", StringType(), True),\n    StructField(\"age\", IntegerType(), True),\n])\n</code></pre> <p>Here's a more complex CSV file:</p> <pre><code>name,type,nullable,metadata\nperson,string,false,{\"description\":\"The person's name\"}\naddress,string\nphoneNumber,string,TRUE,{\"description\":\"The person's phone number\"}\nage,int,False\n</code></pre> <p>Here's how to read this CSV file into a PySpark schema:</p> <pre><code>another_schema = schema_from_csv(spark, \"some_file.csv\")\n\nStructType([\n    StructField(\"person\", StringType(), False, {\"description\": \"The person's name\"}),\n    StructField(\"address\", StringType(), True),\n    StructField(\"phoneNumber\", StringType(), True, {\"description\": \"The person's phone number\"}),\n    StructField(\"age\", IntegerType(), False),\n])\n</code></pre> <p>print_schema_as_code()</p> <pre><code>fields = [\n    StructField(\"simple_int\", IntegerType()),\n    StructField(\"decimal_with_nums\", DecimalType(19, 8)),\n    StructField(\"array\", ArrayType(FloatType()))\n]\nschema = StructType(fields)\nprintable_schema: str = quinn.print_schema_as_code(schema)\n</code></pre> <p>Converts a Spark <code>DataType</code> to a string of Python code that can be evaluated as code using eval(). If the <code>DataType</code> is a <code>StructType</code>, this can be used to print an existing schema in a format that can be copy-pasted into a Python script, log to a file, etc. </p> <p>For example:</p> <pre><code>print(printable_schema)\n</code></pre> <pre><code>StructType(\n    fields=[\n        StructField(\"simple_int\", IntegerType(), True),\n        StructField(\"decimal_with_nums\", DecimalType(19, 8), True),\n        StructField(\n            \"array\",\n            ArrayType(FloatType()),\n            True,\n        ),\n    ]\n)\n</code></pre> <p>Once evaluated, the printable schema is a valid schema that can be used in dataframe creation, validation, etc.</p> <pre><code>from chispa.schema_comparer import assert_basic_schema_equality\n\nparsed_schema = eval(printable_schema)\nassert_basic_schema_equality(parsed_schema, schema) # passes\n</code></pre> <p><code>print_schema_as_code()</code> can also be used to print other <code>DataType</code> objects.</p> <p><code>ArrayType</code></p> <pre><code>array_type = ArrayType(FloatType())\nprintable_type: str = quinn.print_schema_as_code(array_type)\nprint(printable_type)\n ```\n\n ```\nArrayType(FloatType())\n ```\n\n`MapType`\n```python\nmap_type = MapType(StringType(), FloatType())\nprintable_type: str = quinn.print_schema_as_code(map_type)\nprint(printable_type)\n ```\n\n ```\nMapType(\n        StringType(),\n        FloatType(),\n        True,\n)\n ```\n\n`IntegerType`, `StringType` etc.\n```python\ninteger_type = IntegerType()\nprintable_type: str = quinn.print_schema_as_code(integer_type)\nprint(printable_type)\n ```\n\n ```\nIntegerType()\n ```\n\n## Pyspark Core Class Extensions\n\n</code></pre> <p>import pyspark.sql.functions as F import quinn</p> <pre><code>\n### Column Extensions\n\n**isFalsy()**\n\n```python\nsource_df.withColumn(\"is_stuff_falsy\", quinn.is_falsy(F.col(\"has_stuff\")))\n</code></pre> <p>Returns <code>True</code> if <code>has_stuff</code> is <code>None</code> or <code>False</code>.</p> <p>isTruthy()</p> <pre><code>source_df.withColumn(\"is_stuff_truthy\", quinn.is_truthy(F.col(\"has_stuff\")))\n</code></pre> <p>Returns <code>True</code> unless <code>has_stuff</code> is <code>None</code> or <code>False</code>.</p> <p>isNullOrBlank()</p> <pre><code>source_df.withColumn(\"is_blah_null_or_blank\", quinn.is_null_or_blank(F.col(\"blah\")))\n</code></pre> <p>Returns <code>True</code> if <code>blah</code> is <code>null</code> or blank (the empty string or a string that only contains whitespace).</p> <p>isNotIn()</p> <pre><code>source_df.withColumn(\"is_not_bobs_hobby\", quinn.is_not_in(F.col(\"fun_thing\")))\n</code></pre> <p>Returns <code>True</code> if <code>fun_thing</code> is not included in the <code>bobs_hobbies</code> list.</p> <p>nullBetween()</p> <pre><code>source_df.withColumn(\"is_between\", quinn.null_between(F.col(\"age\"), F.col(\"lower_age\"), F.col(\"upper_age\")))\n</code></pre> <p>Returns <code>True</code> if <code>age</code> is between <code>lower_age</code> and <code>upper_age</code>. If <code>lower_age</code> is populated and <code>upper_age</code> is <code>null</code>, it will return <code>True</code> if <code>age</code> is greater than or equal to <code>lower_age</code>. If <code>lower_age</code> is <code>null</code> and <code>upper_age</code> is populate, it will return <code>True</code> if <code>age</code> is lower than or equal to <code>upper_age</code>.</p>"},{"location":"examples/","title":"Examples","text":"<p>Example Quinn code snippets</p> <ul> <li>Schema as Code</li> </ul>"},{"location":"learn_more/","title":"Learn More","text":"<p>Deeper explanations of design decisions and use cases for Quinn</p> <ul> <li>Convert PySpark DataFrame Columns to a Python List</li> </ul>"},{"location":"learn_more/column_to_list/","title":"Column to list performance","text":"<p>In PySpark, there are many approaches to accomplish the same task. Given a test DataFrame containing two columns - mvv and count, here are five methods to produce an identical list of mvv values using base PySpark functionality.</p>"},{"location":"learn_more/column_to_list/#setup","title":"Setup","text":"<pre><code>import pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\n</code></pre> <pre><code>spark = SparkSession.builder.getOrCreate()\nvals = [(0, 5), (1, 10), (2, 4), (3, 2), (4, 1)]\ndf = spark.createDataFrame(count_vals, schema=\"mvv int, count int\")\n</code></pre>"},{"location":"learn_more/column_to_list/#approaches","title":"Approaches","text":""},{"location":"learn_more/column_to_list/#1-topandas","title":"1. toPandas()","text":"<pre><code>list(df.select(\"mvv\").toPandas()[\"mvv\"])\n# [0, 1, 2, 3, 4]\n</code></pre>"},{"location":"learn_more/column_to_list/#2-flatmap","title":"2. flatMap","text":"<pre><code>df.select(\"mvv\").rdd.flatMap(lambda x: x).collect()\n# [0, 1, 2, 3, 4]\n</code></pre>"},{"location":"learn_more/column_to_list/#3-map","title":"3. map","text":"<pre><code>df.select(\"mvv\").rdd.map(lambda row: row[0]).collect()\n# [0, 1, 2, 3, 4]\n</code></pre>"},{"location":"learn_more/column_to_list/#4-collect-list-comprehension","title":"4. collect list comprehension","text":"<pre><code>[row[0] for row in df.select(\"mvv\").collect()]\n# [0, 1, 2, 3, 4]\n</code></pre>"},{"location":"learn_more/column_to_list/#5-tolocaliterator-list-comprehension","title":"5. toLocalIterator() list comprehension","text":"<pre><code>[row[0] for row in df.select(\"mvv\").toLocalIterator()]\n# [0, 1, 2, 3, 4]\n</code></pre>"},{"location":"learn_more/column_to_list/#benchmark-results","title":"Benchmark Results","text":"<p>Substantial runtime differences were observed on the medium and large datasets:</p> <p></p> <p></p> <p>All approaches have similar performance at 1K and 100k rows. <code>toPandas()</code> is consistently the fastest method across the tested dataset sizes, and exhibits the least variance in runtime. However, <code>pyarrow</code> and <code>pandas</code> are not required dependencies of Quinn so this method will only work with those packages available. For typical spark workloads, the <code>flatMap</code> approach is the next best option to use by default.</p>"},{"location":"learn_more/column_to_list/#quinn-implementation","title":"Quinn Implementation","text":"<p> <code>quinn.column_to_list</code></p> <p>To address these performance results, we updated <code>quinn.column_to_list()</code> to check the runtime environment and use the fastest method. If <code>pandas</code> and <code>pyarrow</code> are available, <code>toPandas()</code> is used. Otherwise, <code>flatmap</code> is used.</p>"},{"location":"learn_more/column_to_list/#more-information","title":"More Information","text":""},{"location":"learn_more/column_to_list/#datasets","title":"Datasets","text":"<p>Four datasets were used for this benchmark. Each dataset contains two columns - mvv and index. The mvv column is a monotonically increasing integer and the count column is a random integer between 1 and 10. The datasets were created using the <code>create_benchmark_df.py</code> script in <code>quinn/benchmarks</code></p> Dataset name Number of rows Number of files Size on disk (mb) mvv_xsmall 1,000 1 0.005 mvv_small 100,000 1 0.45 mvv_medium 10,000,000 1 45 mvv_large 100,000,000 4 566"},{"location":"learn_more/column_to_list/#validation","title":"Validation","text":"<p>The code and results from this test are available in the <code>/benchmarks</code> directory of Quinn. To run this benchmark yourself:</p>"},{"location":"learn_more/column_to_list/#1-install-the-required-dependencies","title":"1. install the required dependencies","text":"<pre><code>poetry install --with docs\n</code></pre>"},{"location":"learn_more/column_to_list/#2-create-the-datasets","title":"2. create the datasets","text":"<pre><code>poetry run python benchmarks/create_benchmark_df.py\n</code></pre>"},{"location":"learn_more/column_to_list/#3-run-the-benchmark","title":"3. run the benchmark","text":"<pre><code>poetry run python benchmarks/benchmark_column_performance.py\n</code></pre> <p>Results will be stored in the <code>benchmarks/results</code> directory. By default each implementation will run for the following durations:</p> Dataset name Duration (seconds) mvv_xsmall 20 mvv_small 20 mvv_medium 360 mvv_large 1200 <p>These can be adjusted in benchmarks/benchmark_column_performance.py if a shorter or longer duration is desired.</p>"},{"location":"learn_more/column_to_list/#4-visualize-the-results","title":"4. Visualize the results","text":"<pre><code>poetry run python benchmarks/visualize_benchmarks.py\n</code></pre> <p>.svg files will be saved in the <code>benchmarks/images</code> directory.</p>"},{"location":"notebooks/schema_as_code/","title":"Print SCHEMA as code","text":"In\u00a0[1]: Copied! <pre>from quinn import print_schema_as_code\n</pre> from quinn import print_schema_as_code In\u00a0[2]: Copied! <pre>from pyspark.sql import types as T\n</pre> from pyspark.sql import types as T In\u00a0[3]: Copied! <pre>schema = T.StructType(\n    [\n        T.StructField(\"string_field\", T.StringType()),\n        T.StructField(\"decimal_38_10_field\", T.DecimalType(38, 10)),\n        T.StructField(\"decimal_10_2_field\", T.DecimalType(10, 2)),\n        T.StructField(\"array_of_double\", T.ArrayType(elementType=T.DoubleType())),\n        T.StructField(\"map_type\", T.MapType(keyType=T.StringType(), valueType=T.ShortType())),\n        T.StructField(\"struct_type\", T.StructType([T.StructField(\"t1\", T.StringType()), T.StructField(\"t2\", T.BooleanType())])),\n    ]\n)\n</pre> schema = T.StructType(     [         T.StructField(\"string_field\", T.StringType()),         T.StructField(\"decimal_38_10_field\", T.DecimalType(38, 10)),         T.StructField(\"decimal_10_2_field\", T.DecimalType(10, 2)),         T.StructField(\"array_of_double\", T.ArrayType(elementType=T.DoubleType())),         T.StructField(\"map_type\", T.MapType(keyType=T.StringType(), valueType=T.ShortType())),         T.StructField(\"struct_type\", T.StructType([T.StructField(\"t1\", T.StringType()), T.StructField(\"t2\", T.BooleanType())])),     ] ) In\u00a0[4]: Copied! <pre>print(print_schema_as_code(schema))\n\n# Create a dictionary of PySpark SQL types to provide context to 'eval()' \nspark_type_dict = {k: getattr(T, k) for k in dir(T) if isinstance(getattr(T, k), type)}\neval(print_schema_as_code(schema), {\"__builtins__\": None}, spark_type_dict)\n</pre> print(print_schema_as_code(schema))  # Create a dictionary of PySpark SQL types to provide context to 'eval()'  spark_type_dict = {k: getattr(T, k) for k in dir(T) if isinstance(getattr(T, k), type)} eval(print_schema_as_code(schema), {\"__builtins__\": None}, spark_type_dict) <pre>StructType(\n\tfields=[\n\t\tStructField(\"string_field\", StringType(), True),\n\t\tStructField(\"decimal_38_10_field\", DecimalType(38, 10), True),\n\t\tStructField(\"decimal_10_2_field\", DecimalType(10, 2), True),\n\t\tStructField(\n\t\t\t\"array_of_double\",\n\t\t\tArrayType(DoubleType()),\n\t\t\tTrue,\n\t\t),\n\t\tStructField(\n\t\t\t\"map_type\",\n\t\t\tMapType(\n\t\t\t\tStringType(),\n\t\t\t\tShortType(),\n\t\t\t\tTrue,\n\t\t\t),\n\t\t\tTrue,\n\t\t),\n\t\tStructField(\n\t\t\t\"struct_type\",\n\t\t\tStructType(\n\t\t\t\tfields=[\n\t\t\t\t\tStructField(\"t1\", StringType(), True),\n\t\t\t\t\tStructField(\"t2\", BooleanType(), True),\n\t\t\t\t]\n\t\t\t),\n\t\t\tTrue,\n\t\t),\n\t]\n)\n</pre> Out[4]: <pre>StructType([StructField('string_field', StringType(), True), StructField('decimal_38_10_field', DecimalType(38,10), True), StructField('decimal_10_2_field', DecimalType(10,2), True), StructField('array_of_double', ArrayType(DoubleType(), True), True), StructField('map_type', MapType(StringType(), ShortType(), True), True), StructField('struct_type', StructType([StructField('t1', StringType(), True), StructField('t2', BooleanType(), True)]), True)])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/schema_as_code/#print-schema-as-code","title":"Print SCHEMA as code\u00b6","text":"<p>Function, that take <code>pyspark.sql.types.StructType</code> and print a valid <code>Python</code> code.</p>"},{"location":"reference/SUMMARY/","title":"API Reference","text":"<ul> <li>quinn<ul> <li>append_if_schema_identical</li> <li>dataframe_helpers</li> <li>dataframe_validator</li> <li>extensions<ul> <li>dataframe_ext</li> <li>spark_session_ext</li> </ul> </li> <li>functions</li> <li>keyword_finder</li> <li>math</li> <li>schema_helpers</li> <li>split_columns</li> <li>transformations</li> </ul> </li> </ul>"},{"location":"reference/quinn/","title":"Index","text":"<p>quinn API.</p>"},{"location":"reference/quinn/append_if_schema_identical/","title":"Append if schema identical","text":""},{"location":"reference/quinn/append_if_schema_identical/#quinn.append_if_schema_identical.SchemaMismatchError","title":"<code>SchemaMismatchError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>raise this when there's a schema mismatch between source &amp; target schema.</p> Source code in <code>quinn/append_if_schema_identical.py</code> <pre><code>class SchemaMismatchError(ValueError):\n\"\"\"raise this when there's a schema mismatch between source &amp; target schema.\"\"\"\n</code></pre>"},{"location":"reference/quinn/append_if_schema_identical/#quinn.append_if_schema_identical.append_if_schema_identical","title":"<code>append_if_schema_identical(source_df, target_df)</code>","text":"<p>Compare the schema of source &amp; target dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>source_df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>target_df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>pyspark.sql.DataFrame</code> <p>dataframe</p> Source code in <code>quinn/append_if_schema_identical.py</code> <pre><code>def append_if_schema_identical(source_df: DataFrame, target_df: DataFrame) -&gt; DataFrame:\n\"\"\"Compare the schema of source &amp; target dataframe.\n\n    :param source_df: Input DataFrame\n    :type source_df: pyspark.sql.DataFrame\n    :param target_df: Input DataFrame\n    :type target_df: pyspark.sql.DataFrame\n    :return: dataframe\n    :rtype: pyspark.sql.DataFrame\n    \"\"\"\n    # Retrieve the schemas of the source and target dataframes\n    source_schema = source_df.schema\n    target_schema = target_df.schema\n\n    # Convert the schemas to a list of tuples\n    source_schema_list = [(field.name, str(field.dataType)) for field in source_schema]\n    target_schema_list = [(field.name, str(field.dataType)) for field in target_schema]\n\n    unmatched_cols = [\n        col for col in source_schema_list if col not in target_schema_list\n    ]\n    error_message = (\n        f\"The schemas of the source and target dataframes are not identical.\"\n        f\"From source schema column {unmatched_cols} is missing in target schema\"\n    )\n    # Check if the column names in the source and target schemas are the same, regardless of their order\n    if set(source_schema.fieldNames()) != set(target_schema.fieldNames()):\n        raise SchemaMismatchError(error_message)\n    # Check if the column names and data types in the source and target schemas are the same, in the same order\n    if sorted(source_schema_list) != sorted(target_schema_list):\n        raise SchemaMismatchError(error_message)\n\n    # Append the dataframes if the schemas are identical\n    return target_df.unionByName(source_df)\n</code></pre>"},{"location":"reference/quinn/dataframe_helpers/","title":"Dataframe helpers","text":""},{"location":"reference/quinn/dataframe_helpers/#quinn.dataframe_helpers.column_to_list","title":"<code>column_to_list(df, col_name)</code>","text":"<p>Collect column to list of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>col_name</code> <code>str</code> <p>Column to collect</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of values</p> Source code in <code>quinn/dataframe_helpers.py</code> <pre><code>def column_to_list(df: DataFrame, col_name: str) -&gt; list[Any]:\n\"\"\"Collect column to list of values.\n\n    :param df: Input DataFrame\n    :type df: pyspark.sql.DataFrame\n    :param col_name: Column to collect\n    :type col_name: str\n    :return: List of values\n    :rtype: List[Any]\n    \"\"\"\n    if \"pyspark\" not in sys.modules:\n        raise ImportError\n\n    # sparksession from df is not available in older versions of pyspark\n    if sys.modules[\"pyspark\"].__version__ &lt; \"3.3.0\":\n        return [row[0] for row in df.select(col_name).collect()]\n\n    spark_session = df.sparkSession.getActiveSession()\n    if spark_session is None:\n        return [row[0] for row in df.select(col_name).collect()]\n\n    pyarrow_enabled = (\n        spark_session.conf.get(\n            \"spark.sql.execution.arrow.pyspark.enabled\",\n        )\n        == \"true\"\n    )\n\n    pyarrow_valid = pyarrow_enabled and sys.modules[\"pyarrow\"].__version__ &gt;= \"0.17.0\"\n\n    pandas_exists = \"pandas\" in sys.modules\n    pandas_valid = pandas_exists and sys.modules[\"pandas\"].__version__ &gt;= \"0.24.2\"\n\n    if pyarrow_valid and pandas_valid:\n        return df.select(col_name).toPandas()[col_name].tolist()\n\n    return [row[0] for row in df.select(col_name).collect()]\n</code></pre>"},{"location":"reference/quinn/dataframe_helpers/#quinn.dataframe_helpers.create_df","title":"<code>create_df(spark, rows_data, col_specs)</code>","text":"<p>Create a new DataFrame from the given data and column specs.</p> <p>The returned DataFrame s created using the StructType and StructField classes provided by PySpark.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>SparkSession object</p> required <code>rows_data</code> <code>array-like</code> <p>the data used to create the DataFrame</p> required <code>col_specs</code> <code>list of tuples</code> <p>list of tuples containing the name and type of the field</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>a new DataFrame</p> Source code in <code>quinn/dataframe_helpers.py</code> <pre><code>def create_df(spark: SparkSession, rows_data, col_specs) -&gt; DataFrame:  # noqa: ANN001\n\"\"\"Create a new DataFrame from the given data and column specs.\n\n    The returned DataFrame s created using the StructType and StructField classes provided by PySpark.\n\n    :param spark: SparkSession object\n    :type spark: SparkSession\n    :param rows_data: the data used to create the DataFrame\n    :type rows_data: array-like\n    :param col_specs: list of tuples containing the name and type of the field\n    :type col_specs: list of tuples\n    :return: a new DataFrame\n    :rtype: DataFrame\n    \"\"\"\n    struct_fields = list(map(lambda x: StructField(*x), col_specs))  # noqa: C417\n    return spark.createDataFrame(data=rows_data, schema=StructType(struct_fields))\n</code></pre>"},{"location":"reference/quinn/dataframe_helpers/#quinn.dataframe_helpers.print_athena_create_table","title":"<code>print_athena_create_table(df, athena_table_name, s3location)</code>","text":"<p>Generate the Athena create table statement for a given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pyspark.sql.DataFrame to use</p> required <code>athena_table_name</code> <code>str</code> <p>The name of the athena table to generate</p> required <code>s3location</code> <code>str</code> <p>The S3 location of the parquet data</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>quinn/dataframe_helpers.py</code> <pre><code>def print_athena_create_table(\n    df: DataFrame,\n    athena_table_name: str,\n    s3location: str,\n) -&gt; None:\n\"\"\"Generate the Athena create table statement for a given DataFrame.\n    :param df: The pyspark.sql.DataFrame to use\n    :param athena_table_name: The name of the athena table to generate\n    :param s3location: The S3 location of the parquet data\n    :return: None.\n    \"\"\"\n    warnings.warn(\n        \"Function print_athena_create_table is deprecated and will be removed in the version 1.0\",\n        category=DeprecationWarning,\n        stacklevel=2,\n    )\n\n    fields = df.schema\n\n    print(f\"CREATE EXTERNAL TABLE IF NOT EXISTS `{athena_table_name}` ( \")\n\n    for field in fields.fieldNames()[:-1]:\n        print(\"\\t\", f\"`{fields[field].name}` {fields[field].dataType.simpleString()}, \")\n    last = fields[fields.fieldNames()[-1]]\n    print(\"\\t\", f\"`{last.name}` {last.dataType.simpleString()} \")\n\n    print(\")\")\n    print(\"STORED AS PARQUET\")\n    print(f\"LOCATION '{s3location}'\\n\")\n</code></pre>"},{"location":"reference/quinn/dataframe_helpers/#quinn.dataframe_helpers.show_output_to_df","title":"<code>show_output_to_df(show_output, spark)</code>","text":"<p>Show output as spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>show_output</code> <code>str</code> <p>String representing output of 'show' command in spark</p> required <code>spark</code> <code>SparkSession</code> <p>SparkSession object</p> required <p>Returns:</p> Type Description <code>Dataframe</code> <p>DataFrame object containing output of a show command in spark</p> Source code in <code>quinn/dataframe_helpers.py</code> <pre><code>def show_output_to_df(show_output: str, spark: SparkSession) -&gt; DataFrame:\n\"\"\"Show output as spark DataFrame.\n\n    :param show_output: String representing output of 'show' command in spark\n    :type show_output: str\n    :param spark: SparkSession object\n    :type spark: SparkSession\n    :return: DataFrame object containing output of a show command in spark\n    :rtype: Dataframe\n    \"\"\"\n    lines = show_output.split(\"\\n\")\n    ugly_column_names = lines[1]\n    pretty_column_names = [i.strip() for i in ugly_column_names[1:-1].split(\"|\")]\n    pretty_data = []\n    ugly_data = lines[3:-1]\n    for row in ugly_data:\n        r = [i.strip() for i in row[1:-1].split(\"|\")]\n        pretty_data.append(tuple(r))\n    return spark.createDataFrame(pretty_data, pretty_column_names)\n</code></pre>"},{"location":"reference/quinn/dataframe_helpers/#quinn.dataframe_helpers.to_list_of_dictionaries","title":"<code>to_list_of_dictionaries(df)</code>","text":"<p>Convert a Spark DataFrame to a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Spark DataFrame to convert.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries representing the rows in the DataFrame.</p> Source code in <code>quinn/dataframe_helpers.py</code> <pre><code>def to_list_of_dictionaries(df: DataFrame) -&gt; list[dict[str, Any]]:\n\"\"\"Convert a Spark DataFrame to a list of dictionaries.\n\n    :param df: The Spark DataFrame to convert.\n    :type df: :py:class:`pyspark.sql.DataFrame`\n    :return: A list of dictionaries representing the rows in the DataFrame.\n    :rtype: List[Dict[str, Any]]\n    \"\"\"\n    return list(map(lambda r: r.asDict(), df.collect()))  # noqa: C417\n</code></pre>"},{"location":"reference/quinn/dataframe_helpers/#quinn.dataframe_helpers.two_columns_to_dictionary","title":"<code>two_columns_to_dictionary(df, key_col_name, value_col_name)</code>","text":"<p>Collect two columns as dictionary when first column is key and second is value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>key_col_name</code> <code>str</code> <p>Key-column</p> required <code>value_col_name</code> <code>str</code> <p>Value-column</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with values</p> Source code in <code>quinn/dataframe_helpers.py</code> <pre><code>def two_columns_to_dictionary(\n    df: DataFrame,\n    key_col_name: str,\n    value_col_name: str,\n) -&gt; dict[str, Any]:\n\"\"\"Collect two columns as dictionary when first column is key and second is value.\n\n    :param df: Input DataFrame\n    :type df: pyspark.sql.DataFrame\n    :param key_col_name: Key-column\n    :type key_col_name: str\n    :param value_col_name: Value-column\n    :type value_col_name: str\n    :return: Dictionary with values\n    :rtype: Dict[str, Any]\n    \"\"\"\n    k, v = key_col_name, value_col_name\n    return {x[k]: x[v] for x in df.select(k, v).collect()}\n</code></pre>"},{"location":"reference/quinn/dataframe_validator/","title":"Dataframe validator","text":""},{"location":"reference/quinn/dataframe_validator/#quinn.dataframe_validator.DataFrameMissingColumnError","title":"<code>DataFrameMissingColumnError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>Raise this when there's a DataFrame column error.</p> Source code in <code>quinn/dataframe_validator.py</code> <pre><code>class DataFrameMissingColumnError(ValueError):\n\"\"\"Raise this when there's a DataFrame column error.\"\"\"\n</code></pre>"},{"location":"reference/quinn/dataframe_validator/#quinn.dataframe_validator.DataFrameMissingStructFieldError","title":"<code>DataFrameMissingStructFieldError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>Raise this when there's a DataFrame column error.</p> Source code in <code>quinn/dataframe_validator.py</code> <pre><code>class DataFrameMissingStructFieldError(ValueError):\n\"\"\"Raise this when there's a DataFrame column error.\"\"\"\n</code></pre>"},{"location":"reference/quinn/dataframe_validator/#quinn.dataframe_validator.DataFrameProhibitedColumnError","title":"<code>DataFrameProhibitedColumnError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>Raise this when a DataFrame includes prohibited columns.</p> Source code in <code>quinn/dataframe_validator.py</code> <pre><code>class DataFrameProhibitedColumnError(ValueError):\n\"\"\"Raise this when a DataFrame includes prohibited columns.\"\"\"\n</code></pre>"},{"location":"reference/quinn/dataframe_validator/#quinn.dataframe_validator.validate_absence_of_columns","title":"<code>validate_absence_of_columns(df, prohibited_col_names, return_bool=False)</code>","text":"<p>Validate that none of the prohibited column names are present among specified DataFrame columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing columns to be checked.</p> required <code>prohibited_col_names</code> <code>list[str]</code> <p>List of prohibited column names.</p> required <code>return_bool</code> <code>bool</code> <p>If True, return a boolean instead of raising an exception.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[None, bool]</code> <p>None if return_bool is False, otherwise a boolean indicating if validation passed.</p> <p>Raises:</p> Type Description <code>DataFrameProhibitedColumnError</code> <p>If the prohibited column names are present among the specified DataFrame columns and return_bool is False.</p> Source code in <code>quinn/dataframe_validator.py</code> <pre><code>def validate_absence_of_columns(df: DataFrame, prohibited_col_names: list[str], return_bool: bool = False) -&gt; Union[None, bool]:\n\"\"\"Validate that none of the prohibited column names are present among specified DataFrame columns.\n    :param df: DataFrame containing columns to be checked.\n    :param prohibited_col_names: List of prohibited column names.\n    :param return_bool: If True, return a boolean instead of raising an exception.\n    :type return_bool: bool\n    :return: None if return_bool is False, otherwise a boolean indicating if validation passed.\n    :raises DataFrameProhibitedColumnError: If the prohibited column names are\n    present among the specified DataFrame columns and return_bool is False.\n    \"\"\"\n    all_col_names = df.columns\n    extra_col_names = [x for x in all_col_names if x in prohibited_col_names]\n\n    if extra_col_names:\n        error_message = f\"The {extra_col_names} columns are not allowed to be included in the DataFrame with the following columns {all_col_names}\"\n        if return_bool:\n            return False\n        raise DataFrameProhibitedColumnError(error_message)\n\n    return True if return_bool else None\n</code></pre>"},{"location":"reference/quinn/dataframe_validator/#quinn.dataframe_validator.validate_presence_of_columns","title":"<code>validate_presence_of_columns(df, required_col_names, return_bool=False)</code>","text":"<p>Validate the presence of column names in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A spark DataFrame.</p> required <code>required_col_names</code> <code>list[str]</code> <p>List of the required column names for the DataFrame.</p> required <code>return_bool</code> <code>bool</code> <p>If True, return a boolean instead of raising an exception.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[None, bool]</code> <p>None if return_bool is False, otherwise a boolean indicating if validation passed.</p> <p>Raises:</p> Type Description <code>DataFrameMissingColumnError</code> <p>if any of the requested column names are not present in the DataFrame and return_bool is False.</p> Source code in <code>quinn/dataframe_validator.py</code> <pre><code>def validate_presence_of_columns(df: DataFrame, required_col_names: list[str], return_bool: bool = False) -&gt; Union[None, bool]:\n\"\"\"Validate the presence of column names in a DataFrame.\n    :param df: A spark DataFrame.\n    :type df: DataFrame\n    :param required_col_names: List of the required column names for the DataFrame.\n    :type required_col_names: list[str]\n    :param return_bool: If True, return a boolean instead of raising an exception.\n    :type return_bool: bool\n    :return: None if return_bool is False, otherwise a boolean indicating if validation passed.\n    :raises DataFrameMissingColumnError: if any of the requested column names are\n    not present in the DataFrame and return_bool is False.\n    \"\"\"\n    all_col_names = df.columns\n    missing_col_names = [x for x in required_col_names if x not in all_col_names]\n\n    if missing_col_names:\n        error_message = f\"The {missing_col_names} columns are not included in the DataFrame with the following columns {all_col_names}\"\n        if return_bool:\n            return False\n        raise DataFrameMissingColumnError(error_message)\n\n    return True if return_bool else None\n</code></pre>"},{"location":"reference/quinn/dataframe_validator/#quinn.dataframe_validator.validate_schema","title":"<code>validate_schema(df, required_schema, ignore_nullable=False, return_bool=False)</code>","text":"<p>Function that validate if a given DataFrame has a given StructType as its schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to validate</p> required <code>required_schema</code> <code>StructType</code> <p>StructType required for the DataFrame</p> required <code>ignore_nullable</code> <code>bool</code> <p>(Optional) A flag for if nullable fields should be ignored during validation</p> <code>False</code> <code>return_bool</code> <code>bool</code> <p>If True, return a boolean instead of raising an exception.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[None, bool]</code> <p>None if return_bool is False, otherwise a boolean indicating if validation passed.</p> <p>Raises:</p> Type Description <code>DataFrameMissingStructFieldError</code> <p>if any StructFields from the required schema are not included in the DataFrame schema and return_bool is False.</p> Source code in <code>quinn/dataframe_validator.py</code> <pre><code>def validate_schema(\n    df: DataFrame,\n    required_schema: StructType,\n    ignore_nullable: bool = False,\n    return_bool: bool = False,\n) -&gt; Union[None, bool]:\n\"\"\"Function that validate if a given DataFrame has a given StructType as its schema.\n    :param df: DataFrame to validate\n    :type df: DataFrame\n    :param required_schema: StructType required for the DataFrame\n    :type required_schema: StructType\n    :param ignore_nullable: (Optional) A flag for if nullable fields should be\n    ignored during validation\n    :type ignore_nullable: bool, optional\n    :param return_bool: If True, return a boolean instead of raising an exception.\n    :type return_bool: bool\n    :return: None if return_bool is False, otherwise a boolean indicating if validation passed.\n    :raises DataFrameMissingStructFieldError: if any StructFields from the required\n    schema are not included in the DataFrame schema and return_bool is False.\n    \"\"\"\n    _all_struct_fields = copy.deepcopy(df.schema)\n    _required_schema = copy.deepcopy(required_schema)\n\n    if ignore_nullable:\n        for x in _all_struct_fields:\n            x.nullable = None\n\n        for x in _required_schema:\n            x.nullable = None\n\n    missing_struct_fields = [x for x in _required_schema if x not in _all_struct_fields]\n\n    if missing_struct_fields:\n        error_message = (\n            f\"The {missing_struct_fields} StructFields are not included in the DataFrame with the following StructFields {_all_struct_fields}\"\n        )\n        if return_bool:\n            return False\n        raise DataFrameMissingStructFieldError(error_message)\n\n    return True if return_bool else None\n</code></pre>"},{"location":"reference/quinn/functions/","title":"Functions","text":""},{"location":"reference/quinn/functions/#quinn.functions.anti_trim","title":"<code>anti_trim(col)</code>","text":"<p>Remove all inner whitespace but retain leading and trailing whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>Column on which to perform the regexp_replace.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A new Column with all inner whitespace removed but leading and trailing whitespace retained.</p> Source code in <code>quinn/functions.py</code> <pre><code>def anti_trim(col: Column) -&gt; Column:\n\"\"\"Remove all inner whitespace but retain leading and trailing whitespace.\n\n    :param col: Column on which to perform the regexp_replace.\n    :type col: Column\n    :return: A new Column with all inner whitespace removed but leading and trailing whitespace retained.\n    :rtype: Column\n    \"\"\"\n    return F.regexp_replace(col, \"\\\\b\\\\s+\\\\b\", \"\")\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.approx_equal","title":"<code>approx_equal(col1, col2, threshold)</code>","text":"<p>Compare two <code>Column</code> objects by checking if the difference between them is less than a specified <code>threshold</code>.</p> <p>Parameters:</p> Name Type Description Default <code>col1</code> <code>Column</code> <p>the first <code>Column</code></p> required <code>col2</code> <code>Column</code> <p>the second <code>Column</code></p> required <code>threshold</code> <code>Number</code> <p>value to compare with</p> required <p>Returns:</p> Type Description <code>Column</code> <p>Boolean <code>Column</code> with <code>True</code> indicating that <code>abs(col1 - col2)</code> is less than <code>threshold</code></p> Source code in <code>quinn/functions.py</code> <pre><code>def approx_equal(col1: Column, col2: Column, threshold: Number) -&gt; Column:\n\"\"\"Compare two ``Column`` objects by checking if the difference between them is less than a specified ``threshold``.\n\n    :param col1: the first ``Column``\n    :type col1: Column\n    :param col2: the second ``Column``\n    :type col2: Column\n    :param threshold: value to compare with\n    :type threshold: Number\n    :return: Boolean ``Column`` with ``True`` indicating that ``abs(col1 -\n    col2)`` is less than ``threshold``\n    \"\"\"\n    return F.abs(col1 - col2) &lt; threshold\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.array_choice","title":"<code>array_choice(col, seed=None)</code>","text":"<p>Returns one random element from the given column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>Column from which element is chosen</p> required <p>Returns:</p> Type Description <code>Column</code> <p>random element from the given column</p> Source code in <code>quinn/functions.py</code> <pre><code>def array_choice(col: Column, seed: int | None = None) -&gt; Column:\n\"\"\"Returns one random element from the given column.\n\n    :param col: Column from which element is chosen\n    :type col: Column\n    :return: random element from the given column\n    :rtype: Column\n    \"\"\"\n    index = (F.rand(seed) * F.size(col)).cast(\"int\")\n    return col[index]\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.business_days_between","title":"<code>business_days_between(start_date, end_date)</code>","text":"<p>Function takes two Spark <code>Columns</code> and returns a <code>Column</code> with the number of business days between the start and the end date.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Column</code> <p>The column with the start dates</p> required <code>end_date</code> <code>Column</code> <p>The column with the end dates</p> required <p>Returns:</p> Type Description <code>Column</code> <p>a Column with the number of business days between the start and the end date</p> Source code in <code>quinn/functions.py</code> <pre><code>def business_days_between(\n    start_date: Column,  # noqa: ARG001\n    end_date: Column,  # noqa: ARG001\n) -&gt; Column:\n\"\"\"Function takes two Spark `Columns` and returns a `Column` with the number of business days between the start and the end date.\n\n    :param start_date: The column with the start dates\n    :type start_date: Column\n    :param end_date: The column with the end dates\n    :type end_date: Column\n    :returns: a Column with the number of business days between the start and the end date\n    :rtype: Column\n    \"\"\"\n    all_days = \"sequence(start_date, end_date)\"\n    days_of_week = f\"transform({all_days}, x -&gt; date_format(x, 'E'))\"\n    filter_weekends = F.expr(f\"filter({days_of_week}, x -&gt; x NOT IN ('Sat', 'Sun'))\")\n    num_business_days = F.size(filter_weekends) - 1\n\n    return F.when(num_business_days &lt; 0, None).otherwise(num_business_days)\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.exists","title":"<code>exists(f)</code>","text":"<p>Create a user-defined function.</p> <p>It takes a list expressed as a column of type <code>ArrayType(AnyType)</code> as an argument and returns a boolean value indicating whether any element in the list is true according to the argument <code>f</code> of the <code>exists()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[Any], bool]</code> <p>Callable function - A callable function that takes an element of type Any and returns a boolean value.</p> required <p>Returns:</p> Type Description <code>UserDefinedFunction</code> <p>A user-defined function that takes a list expressed as a column of type ArrayType(AnyType) as an argument and returns a boolean value indicating whether any element in the list is true according to the argument <code>f</code> of the <code>exists()</code> function.</p> Source code in <code>quinn/functions.py</code> <pre><code>def exists(f: Callable[[Any], bool]) -&gt; udf:\n\"\"\"Create a user-defined function.\n\n    It takes a list expressed as a column of type ``ArrayType(AnyType)`` as an argument and returns a boolean value indicating\n    whether any element in the list is true according to the argument ``f`` of the ``exists()`` function.\n\n    :param f: Callable function - A callable function that takes an element of\n    type Any and returns a boolean value.\n    :return: A user-defined function that takes\n    a list expressed as a column of type ArrayType(AnyType) as an argument and\n    returns a boolean value indicating whether any element in the list is true\n    according to the argument ``f`` of the ``exists()`` function.\n    :rtype: UserDefinedFunction\n    \"\"\"\n\n    def temp_udf(list_: list) -&gt; bool:\n        return any(map(f, list_))\n\n    return F.udf(temp_udf, BooleanType())\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.forall","title":"<code>forall(f)</code>","text":"<p>The forall function allows for mapping a given boolean function to a list of arguments and return a single boolean value.</p> <p>It does this by creating a Spark UDF which takes in a list of arguments, applying the given boolean function to each element of the list and returning a single boolean value if all the elements pass through the given boolean function.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[Any], bool]</code> <p>A callable function <code>f</code> which takes in any type and returns a boolean</p> required <p>Returns:</p> Type Description <code>UserDefinedFunction</code> <p>A spark UDF which accepts a list of arguments and returns True if all elements pass through the given boolean function, False otherwise.</p> Source code in <code>quinn/functions.py</code> <pre><code>def forall(f: Callable[[Any], bool]) -&gt; udf:\n\"\"\"The **forall** function allows for mapping a given boolean function to a list of arguments and return a single boolean value.\n\n    It does this by creating a Spark UDF which takes in a list of arguments, applying the given boolean function to\n    each element of the list and returning a single boolean value if all the elements pass through the given boolean function.\n\n    :param f: A callable function ``f`` which takes in any type and returns a boolean\n    :return: A spark UDF which accepts a list of arguments and returns True if all\n    elements pass through the given boolean function, False otherwise.\n    :rtype: UserDefinedFunction\n    \"\"\"\n\n    def temp_udf(list_: list) -&gt; bool:\n        return all(map(f, list_))\n\n    return F.udf(temp_udf, BooleanType())\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.is_false","title":"<code>is_false(col)</code>","text":"<p>Function checks if the column is equal to False and returns the column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>Column</p> required <p>Returns:</p> Type Description <code>Column</code> <p>Column</p> Source code in <code>quinn/functions.py</code> <pre><code>def is_false(col: Column) -&gt; Column:\n\"\"\"Function checks if the column is equal to False and returns the column.\n\n    :param col: Column\n    :return: Column\n    :rtype: Column\n    \"\"\"\n    return col == lit(False)\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.is_falsy","title":"<code>is_falsy(col)</code>","text":"<p>Returns a Column indicating whether all values in the Column are False or NULL (falsy).</p> <p>Each element in the resulting column is True if all the elements in the Column are either NULL or False, or False otherwise. This is accomplished by performing a bitwise or of the <code>isNull</code> condition and a literal False value and then wrapping the result in a when statement.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>Column object</p> required <p>Returns:</p> Type Description <code>Column</code> <p>Column object</p> Source code in <code>quinn/functions.py</code> <pre><code>def is_falsy(col: Column) -&gt; Column:\n\"\"\"Returns a Column indicating whether all values in the Column are False or NULL (**falsy**).\n\n    Each element in the resulting column is True if all the elements in the\n    Column are either NULL or False, or False otherwise. This is accomplished by\n    performing a bitwise or of the ``isNull`` condition and a literal False value and\n    then wrapping the result in a **when** statement.\n\n    :param col: Column object\n    :returns: Column object\n    :rtype: Column\n    \"\"\"\n    return when(col.isNull() | (col == lit(False)), True).otherwise(False)\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.is_not_in","title":"<code>is_not_in(col, _list)</code>","text":"<p>To see if a value is not in a list of values.</p> <p>:_list: list[Any]</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>Column object</p> required Source code in <code>quinn/functions.py</code> <pre><code>def is_not_in(col: Column, _list: list[Any]) -&gt; Column:\n\"\"\"To see if a value is not in a list of values.\n\n    :param col: Column object\n    :_list: list[Any]\n    :rtype: Column\n    \"\"\"\n    return ~(col.isin(_list))\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.is_null_or_blank","title":"<code>is_null_or_blank(col)</code>","text":"<p>Returns a Boolean value which expresses whether a given column is <code>null</code> or contains only blank characters.</p> <p>Parameters:</p> Name Type Description Default <code>\\*\\*col</code> <p>The  :class:<code>Column</code> to check.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A <code>Column</code> containing <code>True</code> if the column is <code>null</code> or only contains blank characters, or <code>False</code> otherwise.</p> Source code in <code>quinn/functions.py</code> <pre><code>def is_null_or_blank(col: Column) -&gt; Column:\nr\"\"\"Returns a Boolean value which expresses whether a given column is ``null`` or contains only blank characters.\n\n    :param \\*\\*col: The  :class:`Column` to check.\n\n    :returns: A `Column` containing ``True`` if the column is ``null`` or only contains\n    blank characters, or ``False`` otherwise.\n    :rtype: Column\n    \"\"\"\n    return (col.isNull()) | (trim(col) == \"\")\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.is_true","title":"<code>is_true(col)</code>","text":"<p>Function takes a column of type Column as an argument and returns a column of type Column.</p> <p>It evaluates whether each element in the column argument is equal to True, and if so will return True, otherwise False.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>Column object</p> required <p>Returns:</p> Type Description <code>Column</code> <p>Column object</p> Source code in <code>quinn/functions.py</code> <pre><code>def is_true(col: Column) -&gt; Column:\n\"\"\"Function takes a column of type Column as an argument and returns a column of type Column.\n\n    It evaluates whether each element in the column argument is equal to True, and\n    if so will return True, otherwise False.\n\n    :param col: Column object\n    :returns: Column object\n    :rtype: Column\n    \"\"\"\n    return col == lit(True)\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.is_truthy","title":"<code>is_truthy(col)</code>","text":"<p>Calculates a boolean expression that is the opposite of is_falsy for the given <code>Column</code> col.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>The <code>Column</code> to calculate the opposite of is_falsy for.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A <code>Column</code> with the results of the calculation.</p> Source code in <code>quinn/functions.py</code> <pre><code>def is_truthy(col: Column) -&gt; Column:\n\"\"\"Calculates a boolean expression that is the opposite of is_falsy for the given ``Column`` col.\n\n    :param Column col: The ``Column`` to calculate the opposite of is_falsy for.\n    :returns: A ``Column`` with the results of the calculation.\n    :rtype: Column\n    \"\"\"\n    return ~(is_falsy(col))\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.multi_equals","title":"<code>multi_equals(value)</code>","text":"<p>Create a user-defined function that checks if all the given columns have the designated value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The designated value.</p> required <p>Returns:</p> Type Description <code>UserDifinedFunction</code> <p>A user-defined function of type BooleanType().</p> Source code in <code>quinn/functions.py</code> <pre><code>def multi_equals(value: Any) -&gt; udf:  # noqa: ANN401\n\"\"\"Create a user-defined function that checks if all the given columns have the designated value.\n\n    :param value: The designated value.\n    :type value: Any\n    :return: A user-defined function of type BooleanType().\n    :rtype: UserDifinedFunction\n    \"\"\"\n\n    def temp_udf(*cols) -&gt; bool:  # noqa: ANN002\n        return all(map(lambda col: col == value, cols))  # noqa: C417\n\n    return F.udf(temp_udf, BooleanType())\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.null_between","title":"<code>null_between(col, lower, upper)</code>","text":"<p>To see if a value is between two values in a null friendly way.</p> <p>:lower: Column :upper: Column</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>Column object</p> required Source code in <code>quinn/functions.py</code> <pre><code>def null_between(col: Column, lower: Column, upper: Column) -&gt; Column:\n\"\"\"To see if a value is between two values in a null friendly way.\n\n    :param col: Column object\n    :lower: Column\n    :upper: Column\n    :rtype: Column\n    \"\"\"\n    return when(lower.isNull() &amp; upper.isNull(), False).otherwise(\n        when(col.isNull(), False).otherwise(\n            when(lower.isNull() &amp; upper.isNotNull() &amp; (col &lt;= upper), True).otherwise(\n                when(\n                    lower.isNotNull() &amp; upper.isNull() &amp; (col &gt;= lower),\n                    True,\n                ).otherwise(col.between(lower, upper)),\n            ),\n        ),\n    )\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.remove_all_whitespace","title":"<code>remove_all_whitespace(col)</code>","text":"<p>Function takes a <code>Column</code> object as a parameter and returns a <code>Column</code> object with all white space removed.</p> <p>It does this using the regexp_replace function from F, which replaces all whitespace with an empty string.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>a <code>Column</code> object</p> required <p>Returns:</p> Type Description <code>Column</code> <p>a <code>Column</code> object with all white space removed</p> Source code in <code>quinn/functions.py</code> <pre><code>def remove_all_whitespace(col: Column) -&gt; Column:\n\"\"\"Function takes a `Column` object as a parameter and returns a `Column` object with all white space removed.\n\n    It does this using the regexp_replace function from F, which replaces all whitespace with an empty string.\n\n    :param col: a `Column` object\n    :type col: Column\n    :returns: a `Column` object with all white space removed\n    :rtype: Column\n    \"\"\"\n    return F.regexp_replace(col, \"\\\\s+\", \"\")\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.remove_non_word_characters","title":"<code>remove_non_word_characters(col)</code>","text":"<p>Removes non-word characters from a column.</p> <p>The non-word characters which will be removed are those identified by the regular expression <code>\"[^\\\\w\\\\s]+\"</code>.  This expression represents any character that is not a word character (e.g. <code>\\\\w</code>) or whitespace (<code>\\\\s</code>).</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>A Column object.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A Column object with non-word characters removed.</p> Source code in <code>quinn/functions.py</code> <pre><code>def remove_non_word_characters(col: Column) -&gt; Column:\nr\"\"\"Removes non-word characters from a column.\n\n    The non-word characters which will be removed are those identified by the\n    regular expression ``\"[^\\\\w\\\\s]+\"``.  This expression represents any character\n    that is not a word character (e.g. `\\\\w`) or whitespace (`\\\\s`).\n\n    :param col: A Column object.\n    :return: A Column object with non-word characters removed.\n\n    \"\"\"\n    return F.regexp_replace(col, \"[^\\\\w\\\\s]+\", \"\")\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.single_space","title":"<code>single_space(col)</code>","text":"<p>Function takes a column and replaces all the multiple white spaces with a single space.</p> <p>It then trims the column to make all the texts consistent.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>The column which needs to be spaced</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A trimmed column with single space</p> Source code in <code>quinn/functions.py</code> <pre><code>def single_space(col: Column) -&gt; Column:\n\"\"\"Function takes a column and replaces all the multiple white spaces with a single space.\n\n    It then trims the column to make all the texts consistent.\n\n    :param col: The column which needs to be spaced\n    :type col: Column\n    :returns: A trimmed column with single space\n    :rtype: Column\n    \"\"\"\n    return F.trim(F.regexp_replace(col, \" +\", \" \"))\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.uuid5","title":"<code>uuid5(col, namespace=uuid.NAMESPACE_DNS, extra_string='')</code>","text":"<p>Function generates UUIDv5 from <code>col</code> and <code>namespace</code>, optionally prepending an extra string to <code>col</code>.</p> <p>Sets variant to RFC 4122 one.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>Column that will be hashed.</p> required <code>namespace</code> <code>uuid.UUID</code> <p>Namespace to be used. (default: <code>uuid.NAMESPACE_DNS</code>)</p> <code>uuid.NAMESPACE_DNS</code> <code>extra_string</code> <code>str</code> <p>In case of collisions one can pass an extra string to hash on.</p> <code>''</code> <p>Returns:</p> Type Description <code>Column</code> <p>String representation of generated UUIDv5</p> Source code in <code>quinn/functions.py</code> <pre><code>def uuid5(\n    col: Column,\n    namespace: uuid.UUID = uuid.NAMESPACE_DNS,\n    extra_string: str = \"\",\n) -&gt; Column:\n\"\"\"Function generates UUIDv5 from ``col`` and ``namespace``, optionally prepending an extra string to ``col``.\n\n    Sets variant to RFC 4122 one.\n\n    :param col: Column that will be hashed.\n    :type col: Column\n    :param namespace: Namespace to be used. (default: `uuid.NAMESPACE_DNS`)\n    :type namespace: str\n    :param extra_string: In case of collisions one can pass an extra string to hash on.\n    :type extra_string: str\n    :return: String representation of generated UUIDv5\n    :rtype: Column\n    \"\"\"\n    ns = F.lit(namespace.bytes)\n    salted_col = F.concat(F.lit(extra_string), col)\n    encoded = F.encode(salted_col, \"utf-8\")\n    encoded_with_ns = F.concat(ns, encoded)\n    hashed = F.sha1(encoded_with_ns)\n    variant_part = F.substring(hashed, 17, 4)\n    variant_part = F.conv(variant_part, 16, 2)\n    variant_part = F.lpad(variant_part, 16, \"0\")\n    variant_part = F.concat(\n        F.lit(\"10\"),\n        F.substring(variant_part, 3, 16),\n    )  # RFC 4122 variant.\n    variant_part = F.lower(F.conv(variant_part, 2, 16))\n    return F.concat_ws(\n        \"-\",\n        F.substring(hashed, 1, 8),\n        F.substring(hashed, 9, 4),\n        F.concat(F.lit(\"5\"), F.substring(hashed, 14, 3)),  # Set version.\n        variant_part,\n        F.substring(hashed, 21, 12),\n    )\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.week_end_date","title":"<code>week_end_date(col, week_end_day='Sat')</code>","text":"<p>Return a date column for the end of week for a given day.</p> <p>The Spark function <code>dayofweek</code> considers Sunday as the first day of the week, and uses the default value of 1 to indicate Sunday. Usage of the <code>when</code> and <code>otherwise</code> functions allow a comparison between the end of week day indicated and the day of week computed, and the return of the reference date if they match or the the addition of one week to the reference date otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>The reference date column.</p> required <code>week_end_day</code> <code>str</code> <p>The week end day (default: 'Sat')</p> <code>'Sat'</code> <p>Returns:</p> Type Description <code>Column</code> <p>A Column of end of the week dates.</p> Source code in <code>quinn/functions.py</code> <pre><code>def week_end_date(col: Column, week_end_day: str = \"Sat\") -&gt; Column:\n\"\"\"Return a date column for the end of week for a given day.\n\n    The Spark function `dayofweek` considers Sunday as the first day of the week, and\n    uses the default value of 1 to indicate Sunday. Usage of the `when` and `otherwise`\n    functions allow a comparison between the end of week day indicated and the day\n    of week computed, and the return of the reference date if they match or the the\n    addition of one week to the reference date otherwise.\n\n    :param col: The reference date column.\n    :type col: Column\n    :param week_end_day: The week end day (default: 'Sat')\n    :type week_end_day: str\n    :return: A Column of end of the week dates.\n    :rtype: Column\n    \"\"\"\n    _raise_if_invalid_day(week_end_day)\n    # these are the default Spark mappings.  Spark considers Sunday the first day of the week.\n    day_of_week_mapping = {\n        \"Sun\": 1,\n        \"Mon\": 2,\n        \"Tue\": 3,\n        \"Wed\": 4,\n        \"Thu\": 5,\n        \"Fri\": 6,\n        \"Sat\": 7,\n    }\n    return F.when(\n        F.dayofweek(col).eqNullSafe(F.lit(day_of_week_mapping[week_end_day])),\n        col,\n    ).otherwise(F.next_day(col, week_end_day))\n</code></pre>"},{"location":"reference/quinn/functions/#quinn.functions.week_start_date","title":"<code>week_start_date(col, week_start_day='Sun')</code>","text":"<p>Function takes a Spark <code>Column</code> and an optional <code>week_start_day</code> argument and returns a <code>Column</code> with the corresponding start of week dates.</p> <p>The \"standard week\" in Spark starts on Sunday, however an optional argument can be used to start the week from a different day, e.g. Monday. The <code>week_start_day</code> argument is a string corresponding to the day of the week to start the week from, e.g. <code>\"Mon\"</code>, <code>\"Tue\"</code>, and must be in the set: <code>{\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"}</code>. If the argument given is not a valid day then a <code>ValueError</code> will be raised.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Column</code> <p>The column to determine start of week dates on</p> required <code>week_start_day</code> <code>str</code> <p>The day to start the week on</p> <code>'Sun'</code> <p>Returns:</p> Type Description <code>Column</code> <p>A Column with start of week dates</p> Source code in <code>quinn/functions.py</code> <pre><code>def week_start_date(col: Column, week_start_day: str = \"Sun\") -&gt; Column:\n\"\"\"Function takes a Spark `Column` and an optional `week_start_day` argument and returns a `Column` with the corresponding start of week dates.\n\n    The \"standard week\" in Spark starts on Sunday, however an optional argument can be\n    used to start the week from a different day, e.g. Monday. The `week_start_day`\n    argument is a string corresponding to the day of the week to start the week\n    from, e.g. `\"Mon\"`, `\"Tue\"`, and must be in the set: `{\"Sun\", \"Mon\", \"Tue\", \"Wed\",\n    \"Thu\", \"Fri\", \"Sat\"}`. If the argument given is not a valid day then a `ValueError`\n    will be raised.\n\n    :param col: The column to determine start of week dates on\n    :type col: Column\n    :param week_start_day: The day to start the week on\n    :type week_start_day: str\n    :returns: A Column with start of week dates\n    :rtype: Column\n    \"\"\"\n    _raise_if_invalid_day(week_start_day)\n    # the \"standard week\" in Spark is from Sunday to Saturday\n    mapping = {\n        \"Sun\": \"Sat\",\n        \"Mon\": \"Sun\",\n        \"Tue\": \"Mon\",\n        \"Wed\": \"Tue\",\n        \"Thu\": \"Wed\",\n        \"Fri\": \"Thu\",\n        \"Sat\": \"Fri\",\n    }\n    end = week_end_date(col, mapping[week_start_day])\n    return F.date_add(end, -6)\n</code></pre>"},{"location":"reference/quinn/keyword_finder/","title":"Keyword finder","text":""},{"location":"reference/quinn/keyword_finder/#quinn.keyword_finder.SearchResult","title":"<code>SearchResult</code>  <code>dataclass</code>","text":"<p>Class to hold the results of a file search. file_path: The path to the file that was searched. word_count: A dictionary containing the number of times each keyword was found in the file.</p> Source code in <code>quinn/keyword_finder.py</code> <pre><code>@dataclass\nclass SearchResult:\n\"\"\"Class to hold the results of a file search.\n    file_path: The path to the file that was searched.\n    word_count: A dictionary containing the number of times each keyword was found in the file.\n    \"\"\"\n\n    file_path: str\n    word_count: dict[str, int]\n</code></pre>"},{"location":"reference/quinn/keyword_finder/#quinn.keyword_finder.keyword_format","title":"<code>keyword_format(input, keywords=default_keywords)</code>","text":"<p>Formats the input string to highlight the keywords.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The string to format.</p> required <code>keywords</code> <code>list[str]</code> <p>The list of keywords to highlight.</p> <code>default_keywords</code> Source code in <code>quinn/keyword_finder.py</code> <pre><code>def keyword_format(input: str, keywords: list[str] = default_keywords) -&gt; str:\n\"\"\"Formats the input string to highlight the keywords.\n\n    :param input: The string to format.\n    :type input: str\n    :param keywords: The list of keywords to highlight.\n    :type keywords: list[str]\n\n    \"\"\"\n    nc = \"\\033[0m\"\n    red = \"\\033[31m\"\n    bold = \"\\033[1m\"\n    res = input\n    for keyword in keywords:\n        res = surround_substring(res, keyword, red + bold, nc)\n    return res\n</code></pre>"},{"location":"reference/quinn/keyword_finder/#quinn.keyword_finder.search_file","title":"<code>search_file(path, keywords=default_keywords)</code>","text":"<p>Searches a file for keywords and prints the line number and line containing the keyword.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file to search.</p> required <code>keywords</code> <code>list[str]</code> <p>The list of keywords to search for.</p> <code>default_keywords</code> <p>Returns:</p> Type Description <code>SearchResult</code> <p>A dictionary containing a file path and the number of lines containing a keyword in <code>keywords</code>.</p> Source code in <code>quinn/keyword_finder.py</code> <pre><code>def search_file(path: str, keywords: list[str] = default_keywords) -&gt; SearchResult:\n\"\"\"Searches a file for keywords and prints the line number and line containing the keyword.\n\n    :param path: The path to the file to search.\n    :type path: str\n    :param keywords: The list of keywords to search for.\n    :type keywords: list[str]\n    :returns: A dictionary containing a file path and the number of lines containing a keyword in `keywords`.\n    :rtype: SearchResult\n\n    \"\"\"\n    match_results = SearchResult(file_path=path, word_count={keyword: 0 for keyword in keywords})\n\n    print(f\"\\nSearching: {path}\")\n    with open(path) as f:\n        for line_number, line in enumerate(f, 1):\n            line_printed = False\n            for keyword in keywords:\n                if keyword in line:\n                    match_results.word_count[keyword] += 1\n\n                    if not line_printed:\n                        print(f\"{line_number}: {keyword_format(line)}\", end=\"\")\n                        line_printed = True\n\n    return match_results\n</code></pre>"},{"location":"reference/quinn/keyword_finder/#quinn.keyword_finder.search_files","title":"<code>search_files(path, keywords=default_keywords)</code>","text":"<p>Searches all files in a directory for keywords.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory to search.</p> required <code>keywords</code> <code>list[str]</code> <p>The list of keywords to search for.</p> <code>default_keywords</code> <p>Returns:</p> Type Description <code>list[SearchResult]</code> <p>A list of dictionaries containing file paths and the number of lines containing a keyword in <code>keywords</code>.</p> Source code in <code>quinn/keyword_finder.py</code> <pre><code>def search_files(path: str, keywords: list[str] = default_keywords) -&gt; list[SearchResult]:\n\"\"\"Searches all files in a directory for keywords.\n\n    :param path: The path to the directory to search.\n    :type path: str\n    :param keywords: The list of keywords to search for.\n    :type keywords: list[str]\n    :returns: A list of dictionaries containing file paths and the number of lines containing a keyword in `keywords`.\n    :rtype: list[SearchResult]\n\n    \"\"\"\n    rootdir_glob = f\"{path}/**/*\"\n    file_list = [f for f in iglob(rootdir_glob, recursive=True) if os.path.isfile(f)]\n    return [search_file(f, keywords) for f in file_list]\n</code></pre>"},{"location":"reference/quinn/keyword_finder/#quinn.keyword_finder.surround_substring","title":"<code>surround_substring(input, substring, surround_start, surround_end)</code>","text":"<p>Surrounds a substring with the given start and end strings.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The string to search.</p> required <code>substring</code> <code>str</code> <p>The substring to surround.</p> required <code>surround_start</code> <code>str</code> <p>The string to start the surrounding with.</p> required <code>surround_end</code> <code>str</code> <p>The string to end the surrounding with.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The input string with the substring surrounded.</p> Source code in <code>quinn/keyword_finder.py</code> <pre><code>def surround_substring(input: str, substring: str, surround_start: str, surround_end: str) -&gt; str:\n\"\"\"Surrounds a substring with the given start and end strings.\n\n    :param input: The string to search.\n    :type input: str\n    :param substring: The substring to surround.\n    :type substring: str\n    :param surround_start: The string to start the surrounding with.\n    :type surround_start: str\n    :param surround_end: The string to end the surrounding with.\n    :type surround_end: str\n    :returns: The input string with the substring surrounded.\n    :rtype: str\n\n    \"\"\"\n    return input.replace(\n        substring,\n        surround_start + substring + surround_end,\n    )\n</code></pre>"},{"location":"reference/quinn/math/","title":"Math","text":"<p>Math routines for PySpark.</p>"},{"location":"reference/quinn/math/#quinn.math.div_or_else","title":"<code>div_or_else(cola, colb, default=0.0)</code>","text":"<p>Return result of division of cola by colb or default if colb is zero.</p> <p>Parameters:</p> Name Type Description Default <code>cola</code> <code>Column</code> <p>dividend</p> required <code>colb</code> <code>Column</code> <p>divisor</p> required <code>default</code> <code>Union[float, Column]</code> <p>default value</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Column</code> <p>result of division or zero</p> Source code in <code>quinn/math.py</code> <pre><code>def div_or_else(\n    cola: Column,\n    colb: Column,\n    default: Union[float, Column] = 0.0,\n) -&gt; Column:\n\"\"\"Return result of division of cola by colb or default if colb is zero.\n\n    :param cola: dividend\n    :param colb: divisor\n    :param default: default value\n    :returns: result of division or zero\n    \"\"\"\n    if not isinstance(default, Column):\n        default = F.lit(default)\n\n    return F.when(colb == F.lit(0.0), default).otherwise(cola / colb)\n</code></pre>"},{"location":"reference/quinn/math/#quinn.math.rand_laplace","title":"<code>rand_laplace(mu, beta, seed=None)</code>","text":"<p>Generate random numbers from Laplace(mu, beta).</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Union[float, Column]</code> <p>mu parameter of Laplace distribution</p> required <code>beta</code> <code>Union[float, Column]</code> <p>beta parameter of Laplace distribution</p> required <code>seed</code> <code>Optional[int]</code> <p>random seed value (optional, default None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Column</code> <p>column with random numbers</p> Source code in <code>quinn/math.py</code> <pre><code>def rand_laplace(\n    mu: Union[float, Column],\n    beta: Union[float, Column],\n    seed: Optional[int] = None,\n) -&gt; Column:\n\"\"\"Generate random numbers from Laplace(mu, beta).\n\n    :param mu: mu parameter of Laplace distribution\n    :param beta: beta parameter of Laplace distribution\n    :param seed: random seed value (optional, default None)\n    :returns: column with random numbers\n    \"\"\"\n    if not isinstance(mu, Column):\n        mu = F.lit(mu)\n\n    if not isinstance(beta, Column):\n        beta = F.lit(beta)\n\n    u = F.rand(seed) - F.lit(0.5)\n    return (mu - beta * F.signum(u) * F.log(F.lit(1) - (F.lit(2) * F.abs(u)))).alias(\n        \"laplace_random\"\n    )\n</code></pre>"},{"location":"reference/quinn/math/#quinn.math.rand_range","title":"<code>rand_range(minimum, maximum, seed=None)</code>","text":"<p>Generate random numbers uniformly distributed in [<code>minimum</code>, <code>maximum</code>).</p> <p>Parameters:</p> Name Type Description Default <code>minimum</code> <code>Union[int, Column]</code> <p>minimum value of the random numbers</p> required <code>maximum</code> <code>Union[int, Column]</code> <p>maximum value of the random numbers</p> required <code>seed</code> <code>Optional[int]</code> <p>random seed value (optional, default None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Column</code> <p>column with random numbers</p> Source code in <code>quinn/math.py</code> <pre><code>def rand_range(\n    minimum: Union[int, Column],\n    maximum: Union[int, Column],\n    seed: Optional[int] = None,\n) -&gt; Column:\n\"\"\"Generate random numbers uniformly distributed in [`minimum`, `maximum`).\n\n    :param minimum: minimum value of the random numbers\n    :param maximum: maximum value of the random numbers\n    :param seed: random seed value (optional, default None)\n    :returns: column with random numbers\n    \"\"\"\n    if not isinstance(minimum, Column):\n        minimum = F.lit(minimum)\n\n    if not isinstance(maximum, Column):\n        maximum = F.lit(maximum)\n\n    u = F.rand(seed)\n\n    return minimum + (maximum - minimum) * u\n</code></pre>"},{"location":"reference/quinn/math/#quinn.math.randn","title":"<code>randn(mean, variance, seed=None)</code>","text":"<p>Generate a column with independent and identically distributed (i.i.d.) samples from the standard normal distribution with given <code>mean</code> and <code>variance</code>..</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Union[float, Column]</code> <p>Mean of the normal distribution of the random numbers</p> required <code>variance</code> <code>Union[float, Column]</code> <p>variance of the normal distribution of the random numbers</p> required <code>seed</code> <code>Optional[int]</code> <p>random seed value (optional, default None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Column</code> <p>column with random numbers</p> Source code in <code>quinn/math.py</code> <pre><code>def randn(\n    mean: Union[float, Column],\n    variance: Union[float, Column],\n    seed: Optional[int] = None,\n) -&gt; Column:\n\"\"\"Generate a column with independent and identically distributed (i.i.d.) samples from\n    the standard normal distribution with given `mean` and `variance`..\n\n    :param mean: Mean of the normal distribution of the random numbers\n    :param variance: variance of the normal distribution of the random numbers\n    :param seed: random seed value (optional, default None)\n    :returns: column with random numbers\n    \"\"\"\n    if not isinstance(mean, Column):\n        mean = F.lit(mean)\n\n    if not isinstance(variance, Column):\n        variance = F.lit(variance)\n\n    return F.randn(seed) * F.sqrt(variance) + mean\n</code></pre>"},{"location":"reference/quinn/schema_helpers/","title":"Schema helpers","text":""},{"location":"reference/quinn/schema_helpers/#quinn.schema_helpers.complex_fields","title":"<code>complex_fields(schema)</code>","text":"<p>Returns a dictionary of complex field names and their data types from the input DataFrame's schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <p>Returns:</p> Type Description <code>Dict[str, object]</code> <p>A dictionary with complex field names as keys and their respective data types as values.</p> Source code in <code>quinn/schema_helpers.py</code> <pre><code>def complex_fields(schema: T.StructType) -&gt; dict[str, object]:\n\"\"\"Returns a dictionary of complex field names and their data types from the input DataFrame's schema.\n\n    :param df: The input PySpark DataFrame.\n    :type df: DataFrame\n    :return: A dictionary with complex field names as keys and their respective data types as values.\n    :rtype: Dict[str, object]\n    \"\"\"\n    return {\n        field.name: field.dataType\n        for field in schema.fields\n        if isinstance(field.dataType, (T.ArrayType, T.StructType, T.MapType))\n    }\n</code></pre>"},{"location":"reference/quinn/schema_helpers/#quinn.schema_helpers.print_schema_as_code","title":"<code>print_schema_as_code(dtype)</code>","text":"<p>Represent DataType (including StructType) as valid Python code.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>T.DataType</code> <p>The input DataType or Schema object</p> required <p>Returns:</p> Type Description <code>str</code> <p>A valid python code which generate the same schema.</p> Source code in <code>quinn/schema_helpers.py</code> <pre><code>def print_schema_as_code(dtype: T.DataType) -&gt; str:\n\"\"\"Represent DataType (including StructType) as valid Python code.\n\n    :param dtype: The input DataType or Schema object\n    :type dtype: pyspark.sql.types.DataType\n    :return: A valid python code which generate the same schema.\n    :rtype: str\n    \"\"\"\n    res = []\n    if isinstance(dtype, T.StructType):\n        res.append(\"StructType(\\n\\tfields=[\")\n        for field in dtype.fields:\n            for line in _repr_column(field).split(\"\\n\"):\n                res.append(\"\\n\\t\\t\")\n                res.append(line)\n            res.append(\",\")\n        res.append(\"\\n\\t]\\n)\")\n\n    elif isinstance(dtype, T.ArrayType):\n        res.append(\"ArrayType(\")\n        res.append(print_schema_as_code(dtype.elementType))\n        res.append(\")\")\n\n    elif isinstance(dtype, T.MapType):\n        res.append(\"MapType(\")\n        res.append(f\"\\n\\t{print_schema_as_code(dtype.keyType)},\")\n        for line in print_schema_as_code(dtype.valueType).split(\"\\n\"):\n            res.append(\"\\n\\t\")\n            res.append(line)\n        res.append(\",\")\n        res.append(f\"\\n\\t{dtype.valueContainsNull},\")\n        res.append(\"\\n)\")\n\n    elif isinstance(dtype, T.DecimalType):\n        res.append(f\"DecimalType({dtype.precision}, {dtype.scale})\")\n\n    elif str(dtype).endswith(\"()\"):\n        # PySpark 3.3+\n        res.append(str(dtype))\n    else:\n        res.append(f\"{dtype}()\")\n\n    return \"\".join(res)\n</code></pre>"},{"location":"reference/quinn/schema_helpers/#quinn.schema_helpers.schema_from_csv","title":"<code>schema_from_csv(spark, file_path)</code>","text":"<p>Return a StructType from a CSV file containing schema configuration.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The SparkSession object</p> required <code>file_path</code> <code>str</code> <p>The path to the CSV file containing the schema configuration</p> required <p>Returns:</p> Type Description <code>pyspark.sql.types.StructType</code> <p>A StructType object representing the schema configuration</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the CSV file does not contain the expected columns: name, type, nullable, description</p> Source code in <code>quinn/schema_helpers.py</code> <pre><code>def schema_from_csv(spark: SparkSession, file_path: str) -&gt; T.StructType:  # noqa: C901\n\"\"\"Return a StructType from a CSV file containing schema configuration.\n\n    :param spark: The SparkSession object\n    :type spark: pyspark.sql.session.SparkSession\n\n    :param file_path: The path to the CSV file containing the schema configuration\n    :type file_path: str\n\n    :raises ValueError: If the CSV file does not contain the expected columns: name, type, nullable, description\n\n    :return: A StructType object representing the schema configuration\n    :rtype: pyspark.sql.types.StructType\n    \"\"\"\n\n    def _validate_json(metadata: Optional[str]) -&gt; dict:\n        if metadata is None:\n            return {}\n\n        try:\n            metadata_dict = json.loads(metadata)\n\n        except json.JSONDecodeError as exc:\n            msg = f\"Invalid JSON: {metadata}\"\n            raise ValueError(msg) from exc\n\n        return metadata_dict\n\n    def _lookup_type(type_str: str) -&gt; T.DataType:\n        type_lookup = {\n            \"string\": T.StringType(),\n            \"int\": T.IntegerType(),\n            \"float\": T.FloatType(),\n            \"double\": T.DoubleType(),\n            \"boolean\": T.BooleanType(),\n            \"bool\": T.BooleanType(),\n            \"timestamp\": T.TimestampType(),\n            \"date\": T.DateType(),\n            \"binary\": T.BinaryType(),\n        }\n\n        if type_str not in type_lookup:\n            msg = f\"Invalid type: {type_str}. Expecting one of: {type_lookup.keys()}\"\n            raise ValueError(msg)\n\n        return type_lookup[type_str]\n\n    def _convert_nullable(null_str: str) -&gt; bool:\n        if null_str is None:\n            return True\n\n        parsed_val = null_str.lower()\n        if parsed_val not in [\"true\", \"false\"]:\n            msg = f\"Invalid nullable value: {null_str}. Expecting True or False.\"\n            raise ValueError(msg)\n\n        return parsed_val == \"true\"\n\n    schema_df = spark.read.csv(file_path, header=True)\n    possible_columns = [\"name\", \"type\", \"nullable\", \"metadata\"]\n    num_cols = len(schema_df.columns)\n    expected_columns = possible_columns[0:num_cols]\n\n    # ensure that csv contains the expected columns: name, type, nullable, description\n    if schema_df.columns != expected_columns:\n        msg = f\"CSV must contain columns in this order: {expected_columns}\"\n        raise ValueError(msg)\n\n    # create a StructType per field\n    fields = []\n    for row in schema_df.collect():\n        field = T.StructField(\n            name=row[\"name\"],\n            dataType=_lookup_type(row[\"type\"]),\n            nullable=_convert_nullable(row[\"nullable\"]) if \"nullable\" in row else True,\n            metadata=_validate_json(row[\"metadata\"] if \"metadata\" in row else None),\n        )\n        fields.append(field)\n\n    return T.StructType(fields=fields)\n</code></pre>"},{"location":"reference/quinn/split_columns/","title":"Split columns","text":""},{"location":"reference/quinn/split_columns/#quinn.split_columns.split_col","title":"<code>split_col(df, col_name, delimiter, new_col_names, mode='permissive', default=None)</code>","text":"<p>Splits the given column based on the delimiter and creates new columns with the split values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame</p> required <code>col_name</code> <code>str</code> <p>The name of the column to split</p> required <code>delimiter</code> <code>str</code> <p>The delimiter to split the column on</p> required <code>new_col_names</code> <code>list[str]</code> <p>A list of two strings for the new column names</p> required <code>mode</code> <code>str</code> <p>The split mode. Can be \"strict\" or \"permissive\". Default is \"permissive\"</p> <code>'permissive'</code> <code>default</code> <code>Optional[str]</code> <p>If the mode is \"permissive\" then default value will be assigned to column</p> <code>None</code> <p>Returns:</p> Type Description <code>pyspark.sql.DataFrame.</code> <p>dataframe: The resulting DataFrame with the split columns</p> Source code in <code>quinn/split_columns.py</code> <pre><code>def split_col(  # noqa: PLR0913\n    df: DataFrame,\n    col_name: str,\n    delimiter: str,\n    new_col_names: list[str],\n    mode: str = \"permissive\",\n    default: Optional[str] = None,\n) -&gt; DataFrame:\n\"\"\"Splits the given column based on the delimiter and creates new columns with the split values.\n\n    :param df: The input DataFrame\n    :type df: pyspark.sql.DataFrame\n    :param col_name: The name of the column to split\n    :type col_name: str\n    :param delimiter: The delimiter to split the column on\n    :type delimiter: str\n    :param new_col_names: A list of two strings for the new column names\n    :type new_col_names: (List[str])\n    :param mode: The split mode. Can be \"strict\" or \"permissive\". Default is \"permissive\"\n    :type mode: str\n    :param default: If the mode is \"permissive\" then default value will be assigned to column\n    :type mode: str\n    :return: dataframe: The resulting DataFrame with the split columns\n    :rtype: pyspark.sql.DataFrame.\n    \"\"\"\n    # Check if the column to be split exists in the DataFrame\n    if col_name not in df.columns:\n        msg = f\"Column '{col_name}' not found in DataFrame.\"\n        raise ValueError(msg)\n\n    # Check if the delimiter is a string\n    if not isinstance(delimiter, str):\n        msg = \"Delimiter must be a string.\"\n        raise TypeError(msg)\n\n    # Check if the new column names are a list of strings\n    if not isinstance(new_col_names, list):\n        msg = \"New column names must be a list of strings.\"\n        raise TypeError(msg)\n\n    # Define a UDF to check the occurrence of delimitter\n    def _num_delimiter(col_value1: str) -&gt; int:\n        # Get the count of delimiter and store the result in no_of_delimiter\n        no_of_delimiter = col_value1.count(delimiter)\n        # Split col_value based on delimiter and store the result in split_value\n        split_value = col_value1.split(delimiter)\n\n        # Check if col_value is not None\n        if col_value1 is not None:\n            # Check if the no of delimiters in split_value is not as expected\n            if no_of_delimiter != len(new_col_names) - 1:\n                # If the length is not same, raise an IndexError with the message mentioning the expected and found length\n                msg = f\"Expected {len(new_col_names)} elements after splitting on delimiter, found {len(split_value)} elements\"\n                raise IndexError(\n                    msg,\n                )\n\n            # If the length of split_value is same as new_col_names, check if any of the split values is None or empty string\n            elif any(  # noqa: RET506\n                x is None or x.strip() == \"\" for x in split_value[: len(new_col_names)]\n            ):\n                msg = \"Null or empty values are not accepted for columns in strict mode\"\n                raise ValueError(\n                    msg,\n                )\n\n            # If the above checks pass, return the count of delimiter\n            return int(no_of_delimiter)\n\n        # If col_value is None, return 0\n        return 0\n\n    num_udf = udf(lambda y: None if y is None else _num_delimiter(y), IntegerType())\n\n    # Get the column expression for the column to be split\n    col_expr = df[col_name]\n\n    # Split the column by the delimiter\n    split_col_expr = split(trim(col_expr), delimiter)\n\n    # Check the split mode\n    if mode == \"strict\":\n        # Create an array of select expressions to create new columns from the split values\n        select_exprs = [\n            when(split_col_expr.getItem(i) != \"\", split_col_expr.getItem(i)).alias(\n                new_col_names[i],\n            )\n            for i in range(len(new_col_names))\n        ]\n\n        # Select all the columns from the input DataFrame, along with the new split columns\n        df = df.select(\"*\", *select_exprs)  # noqa: PD901\n        df = df.withColumn(\"del_length\", num_udf(df[col_name]))  # noqa: PD901\n        df.cache()\n        # Drop the original column if the new columns were created successfully\n        df = df.select( # noqa: PD901\n            [c for c in df.columns if c not in {\"del_length\", col_name}],\n        )\n\n    elif mode == \"permissive\":\n        # Create an array of select expressions to create new columns from the split values\n        # Use the default value if a split value is missing or empty\n        select_exprs = select_exprs = [\n            when(length(split_col_expr.getItem(i)) &gt; 0, split_col_expr.getItem(i))\n            .otherwise(default)\n            .alias(new_col_names[i])\n            for i in range(len(new_col_names))\n        ]\n\n        # Select all the columns from the input DataFrame, along with the new split columns\n        # Drop the original column if the new columns were created successfully\n        df = df.select(\"*\", *select_exprs).drop(col_name)  # noqa: PD901\n        df.cache()\n\n    else:\n        msg = f\"Invalid mode: {mode}\"\n        raise ValueError(msg)\n\n    # Return the DataFrame with the split columns\n    return df\n</code></pre>"},{"location":"reference/quinn/transformations/","title":"Transformations","text":""},{"location":"reference/quinn/transformations/#quinn.transformations.flatten_dataframe","title":"<code>flatten_dataframe(df, separator=':', replace_char='_', sanitized_columns=False)</code>","text":"<p>Flattens the complex columns in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>separator</code> <code>str</code> <p>The separator to use in the resulting flattened column names, defaults to \":\".</p> <code>':'</code> <code>replace_char</code> <code>str</code> <p>The character to replace special characters with in column names, defaults to \"_\".</p> <code>'_'</code> <code>sanitized_columns</code> <code>bool</code> <p>Whether to sanitize column names, defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame  .. note:: This function assumes the input DataFrame has a consistent schema across all rows. If you have files with different schemas, process each separately instead.  .. example:: Example usage:  &gt;&gt;&gt; data = [ ( 1, (\"Alice\", 25), {\"A\": 100, \"B\": 200}, [\"apple\", \"banana\"], {\"key\": {\"nested_key\": 10}}, {\"A#\": 1000, \"B@\": 2000}, ), ( 2, (\"Bob\", 30), {\"A\": 150, \"B\": 250}, [\"orange\", \"grape\"], {\"key\": {\"nested_key\": 20}}, {\"A#\": 1500, \"B@\": 2500}, ), ]  &gt;&gt;&gt; df = spark.createDataFrame(data) &gt;&gt;&gt; flattened_df = flatten_dataframe(df) &gt;&gt;&gt; flattened_df.show() &gt;&gt;&gt; flattened_df_with_hyphen = flatten_dataframe(df, replace_char=\"-\") &gt;&gt;&gt; flattened_df_with_hyphen.show()</code> <p>The DataFrame with all complex data types flattened.</p> Source code in <code>quinn/transformations.py</code> <pre><code>def flatten_dataframe(\n    df: DataFrame,\n    separator: str = \":\",\n    replace_char: str = \"_\",\n    sanitized_columns: bool = False,\n) -&gt; DataFrame:\n\"\"\"Flattens the complex columns in the DataFrame.\n\n    :param df: The input PySpark DataFrame.\n    :type df: DataFrame\n    :param separator: The separator to use in the resulting flattened column names, defaults to \":\".\n    :type separator: str, optional\n    :param replace_char: The character to replace special characters with in column names, defaults to \"_\".\n    :type replace_char: str, optional\n    :param sanitized_columns: Whether to sanitize column names, defaults to False.\n    :type sanitized_columns: bool, optional\n    :return: The DataFrame with all complex data types flattened.\n    :rtype: DataFrame\n\n    .. note:: This function assumes the input DataFrame has a consistent schema across all rows. If you have files with\n        different schemas, process each separately instead.\n\n    .. example:: Example usage:\n\n        &gt;&gt;&gt; data = [\n                (\n                    1,\n                    (\"Alice\", 25),\n                    {\"A\": 100, \"B\": 200},\n                    [\"apple\", \"banana\"],\n                    {\"key\": {\"nested_key\": 10}},\n                    {\"A#\": 1000, \"B@\": 2000},\n                ),\n                (\n                    2,\n                    (\"Bob\", 30),\n                    {\"A\": 150, \"B\": 250},\n                    [\"orange\", \"grape\"],\n                    {\"key\": {\"nested_key\": 20}},\n                    {\"A#\": 1500, \"B@\": 2500},\n                ),\n            ]\n\n        &gt;&gt;&gt; df = spark.createDataFrame(data)\n        &gt;&gt;&gt; flattened_df = flatten_dataframe(df)\n        &gt;&gt;&gt; flattened_df.show()\n        &gt;&gt;&gt; flattened_df_with_hyphen = flatten_dataframe(df, replace_char=\"-\")\n        &gt;&gt;&gt; flattened_df_with_hyphen.show()\n    \"\"\"\n\n    def sanitize_column_name(name: str, rc: str = \"_\") -&gt; str:\n\"\"\"Sanitizes column names by replacing special characters with the specified character.\n\n        :param name: The original column name.\n        :type name: str\n        :param rc: The character to replace special characters with, defaults to '_'.\n        :type rc: str, optional\n        :return: The sanitized column name.\n        :rtype: str\n        \"\"\"\n        return re.sub(r\"[^a-zA-Z0-9_]\", rc, name)\n\n    def explode_array(df: DataFrame, col_name: str) -&gt; DataFrame:\n\"\"\"Explodes the specified ArrayType column in the input DataFrame and returns a new DataFrame with the exploded column.\n\n        :param df: The input PySpark DataFrame.\n        :type df: DataFrame\n        :param col_name: The column name of the ArrayType to be exploded.\n        :type col_name: str\n        :return: The DataFrame with the exploded ArrayType column.\n        :rtype: DataFrame\n        \"\"\"\n        return df.select(\n            \"*\",\n            F.explode_outer(F.col(f\"`{col_name}`\")).alias(col_name),\n        ).drop(\n            col_name,\n        )\n\n    fields = complex_fields(df.schema)\n\n    while len(fields) != 0:\n        col_name = next(iter(fields.keys()))\n\n        if isinstance(fields[col_name], StructType):\n            df = flatten_struct(df, col_name, separator)  # noqa: PD901\n\n        elif isinstance(fields[col_name], ArrayType):\n            df = explode_array(df, col_name)  # noqa: PD901\n\n        elif isinstance(fields[col_name], MapType):\n            df = flatten_map(df, col_name, separator)  # noqa: PD901\n\n        fields = complex_fields(df.schema)\n\n    # Sanitize column names with the specified replace_char\n    if sanitized_columns:\n        sanitized_columns = [\n            sanitize_column_name(col_name, replace_char) for col_name in df.columns\n        ]\n        df = df.toDF(*sanitized_columns)  # noqa: PD901\n\n    return df\n</code></pre>"},{"location":"reference/quinn/transformations/#quinn.transformations.flatten_map","title":"<code>flatten_map(df, col_name, separator=':')</code>","text":"<p>Flattens the specified MapType column in the input DataFrame and returns a new DataFrame with the flattened columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>col_name</code> <code>str</code> <p>The column name of the MapType to be flattened.</p> required <code>separator</code> <code>str</code> <p>The separator to use in the resulting flattened column names, defaults to \":\".</p> <code>':'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The DataFrame with the flattened MapType column.</p> Source code in <code>quinn/transformations.py</code> <pre><code>def flatten_map(df: DataFrame, col_name: str, separator: str = \":\") -&gt; DataFrame:\n\"\"\"Flattens the specified MapType column in the input DataFrame and returns a new DataFrame with the flattened columns.\n\n    :param df: The input PySpark DataFrame.\n    :type df: DataFrame\n    :param col_name: The column name of the MapType to be flattened.\n    :type col_name: str\n    :param separator: The separator to use in the resulting flattened column names, defaults to \":\".\n    :type separator: str, optional\n    :return: The DataFrame with the flattened MapType column.\n    :rtype: DataFrame\n    \"\"\"\n    keys_df = df.select(F.explode_outer(F.map_keys(F.col(f\"`{col_name}`\")))).distinct()\n    keys = [row[0] for row in keys_df.collect()]\n    key_cols = [\n        F.col(f\"`{col_name}`\").getItem(k).alias(col_name + separator + k) for k in keys\n    ]\n    return df.select(\n        [F.col(f\"`{col}`\") for col in df.columns if col != col_name] + key_cols,\n    )\n</code></pre>"},{"location":"reference/quinn/transformations/#quinn.transformations.flatten_struct","title":"<code>flatten_struct(df, col_name, separator=':')</code>","text":"<p>Flattens the specified StructType column in the input DataFrame and returns a new DataFrame with the flattened columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>col_name</code> <code>str</code> <p>The column name of the StructType to be flattened.</p> required <code>separator</code> <code>str</code> <p>The separator to use in the resulting flattened column names, defaults to ':'.</p> <code>':'</code> <p>Returns:</p> Type Description <code>List[Column]</code> <p>The DataFrame with the flattened StructType column.</p> Source code in <code>quinn/transformations.py</code> <pre><code>def flatten_struct(df: DataFrame, col_name: str, separator: str = \":\") -&gt; DataFrame:\n\"\"\"Flattens the specified StructType column in the input DataFrame and returns a new DataFrame with the flattened columns.\n\n    :param df: The input PySpark DataFrame.\n    :type df: DataFrame\n    :param col_name: The column name of the StructType to be flattened.\n    :type col_name: str\n    :param separator: The separator to use in the resulting flattened column names, defaults to ':'.\n    :type separator: str, optional\n    :return: The DataFrame with the flattened StructType column.\n    :rtype: List[Column]\n    \"\"\"\n    struct_type = complex_fields(df.schema)[col_name]\n    expanded = [\n        F.col(f\"`{col_name}`.`{k}`\").alias(col_name + separator + k)\n        for k in [n.name for n in struct_type.fields]\n    ]\n    return df.select(\"*\", *expanded).drop(F.col(f\"`{col_name}`\"))\n</code></pre>"},{"location":"reference/quinn/transformations/#quinn.transformations.snake_case_col_names","title":"<code>snake_case_col_names(df)</code>","text":"<p>Function takes a <code>DataFrame</code> instance and returns the same <code>DataFrame</code> instance with all column names converted to snake case.</p> <p>(e.g. <code>col_name_1</code>). It uses the <code>to_snake_case</code> function in conjunction with the <code>with_columns_renamed</code> function to achieve this.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A <code>DataFrame</code> instance to process</p> required <p>Returns:</p> Type Description <code>``DataFrame``.</code> <p>A <code>DataFrame</code> instance with column names converted to snake case</p> Source code in <code>quinn/transformations.py</code> <pre><code>def snake_case_col_names(df: DataFrame) -&gt; DataFrame:\n\"\"\"Function takes a ``DataFrame`` instance and returns the same ``DataFrame`` instance with all column names converted to snake case.\n\n    (e.g. ``col_name_1``). It uses the ``to_snake_case`` function in conjunction with\n    the ``with_columns_renamed`` function to achieve this.\n    :param df: A ``DataFrame`` instance to process\n    :type df: ``DataFrame``\n    :return: A ``DataFrame`` instance with column names converted to snake case\n    :rtype: ``DataFrame``.\n    \"\"\"\n    return with_columns_renamed(to_snake_case)(df)\n</code></pre>"},{"location":"reference/quinn/transformations/#quinn.transformations.sort_columns","title":"<code>sort_columns(df, sort_order, sort_nested=False)</code>","text":"<p>This function sorts the columns of a given DataFrame based on a given sort order. The <code>sort_order</code> parameter can either be <code>asc</code> or <code>desc</code>, which correspond to ascending and descending order, respectively. If any other value is provided for the <code>sort_order</code> parameter, a <code>ValueError</code> will be raised.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame</p> required <code>sort_order</code> <code>str</code> <p>The order in which to sort the columns in the DataFrame</p> required <code>sort_nested</code> <code>bool</code> <p>Whether to sort nested structs or not. Defaults to false.</p> <code>False</code> <p>Returns:</p> Type Description <code>pyspark.sql.DataFrame</code> <p>A DataFrame with the columns sorted in the chosen order</p> Source code in <code>quinn/transformations.py</code> <pre><code>def sort_columns(  # noqa: C901,PLR0915\n    df: DataFrame,\n    sort_order: str,\n    sort_nested: bool = False,\n) -&gt; DataFrame:\n\"\"\"This function sorts the columns of a given DataFrame based on a given sort\n    order. The ``sort_order`` parameter can either be ``asc`` or ``desc``, which correspond to\n    ascending and descending order, respectively. If any other value is provided for\n    the ``sort_order`` parameter, a ``ValueError`` will be raised.\n\n    :param df: A DataFrame\n    :type df: pyspark.sql.DataFrame\n    :param sort_order: The order in which to sort the columns in the DataFrame\n    :type sort_order: str\n    :param sort_nested: Whether to sort nested structs or not. Defaults to false.\n    :type sort_nested: bool\n    :return: A DataFrame with the columns sorted in the chosen order\n    :rtype: pyspark.sql.DataFrame\n    \"\"\"\n\n    def sort_nested_cols(\n        schema: StructType, is_reversed: bool, base_field: str=\"\",\n    ) -&gt; list[str]:\n        # recursively check nested fields and sort them\n        # https://stackoverflow.com/questions/57821538/how-to-sort-columns-of-nested-structs-alphabetically-in-pyspark\n        # Credits: @pault for logic\n\n        def parse_fields(\n            fields_to_sort: list,\n            parent_struct: StructType,\n            is_reversed: bool,\n        ) -&gt; list:\n            sorted_fields: list = sorted(\n                fields_to_sort,\n                key=lambda x: x[\"name\"],\n                reverse=is_reversed,\n            )\n\n            results = []\n            for field in sorted_fields:\n                new_struct = StructType([StructField.fromJson(field)])\n                new_base_field = parent_struct.name\n                if base_field:\n                    new_base_field = base_field + \".\" + new_base_field\n\n                results.extend(\n                    sort_nested_cols(\n                        new_struct, is_reversed, base_field=new_base_field,\n                    ),\n                )\n            return results\n\n        select_cols = []\n        for parent_struct in sorted(schema, key=lambda x: x.name, reverse=is_reversed):\n            field_type = parent_struct.dataType\n            if isinstance(field_type, ArrayType):\n                array_parent = parent_struct.jsonValue()[\"type\"][\"elementType\"]\n                base_str = f\"transform({parent_struct.name}\"\n                suffix_str = f\") AS {parent_struct.name}\"\n\n                # if struct in array, create mapping to struct\n                if array_parent[\"type\"] == \"struct\":\n                    array_parent = array_parent[\"fields\"]\n                    base_str = f\"{base_str}, x -&gt; struct(\"\n                    suffix_str = f\"){suffix_str}\"\n\n                array_elements = parse_fields(array_parent, parent_struct, is_reversed)\n                element_names = [i.split(\".\")[-1] for i in array_elements]\n                array_elements_formatted = [f\"x.{i} as {i}\" for i in element_names]\n\n                # create a string representation of the sorted array\n                # ex: transform(phone_numbers, x -&gt; struct(x.number as number, x.type as type)) AS phone_numbers\n                result = f\"{base_str}{', '.join(array_elements_formatted)}{suffix_str}\"\n\n            elif isinstance(field_type, StructType):\n                field_list = parent_struct.jsonValue()[\"type\"][\"fields\"]\n                sub_fields = parse_fields(field_list, parent_struct, is_reversed)\n\n                # create a string representation of the sorted struct\n                # ex: struct(address.zip.first5, address.zip.last4) AS zip\n                result = f\"struct({', '.join(sub_fields)}) AS {parent_struct.name}\"\n\n            elif base_field:\n                result = f\"{base_field}.{parent_struct.name}\"\n            else:\n                result = parent_struct.name\n            select_cols.append(result)\n\n        return select_cols\n\n    def get_original_nullability(field: StructField, result_dict: dict) -&gt; None:\n        if hasattr(field, \"nullable\"):\n            result_dict[field.name] = field.nullable\n        else:\n            result_dict[field.name] = True\n\n        if not isinstance(field.dataType, StructType) and not isinstance(\n            field.dataType,\n            ArrayType,\n        ):\n            return\n\n        if isinstance(field.dataType, ArrayType):\n            result_dict[f\"{field.name}_element\"] = field.dataType.containsNull\n            children = field.dataType.elementType.fields\n        else:\n            children = field.dataType.fields\n        for i in children:\n            get_original_nullability(i, result_dict)\n\n    def fix_nullability(field: StructField, result_dict: dict) -&gt; None:\n        field.nullable = result_dict[field.name]\n        if not isinstance(field.dataType, StructType) and not isinstance(\n            field.dataType,\n            ArrayType,\n        ):\n            return\n\n        if isinstance(field.dataType, ArrayType):\n            # save the containsNull property of the ArrayType\n            field.dataType.containsNull = result_dict[f\"{field.name}_element\"]\n            children = field.dataType.elementType.fields\n        else:\n            children = field.dataType.fields\n\n        for i in children:\n            fix_nullability(i, result_dict)\n\n    if sort_order not in [\"asc\", \"desc\"]:\n        msg = f\"['asc', 'desc'] are the only valid sort orders and you entered a sort order of '{sort_order}'\"\n        raise ValueError(\n            msg,\n        )\n    reverse_lookup = {\n        \"asc\": False,\n        \"desc\": True,\n    }\n\n    is_reversed: bool = reverse_lookup[sort_order]\n    top_level_sorted_df = df.select(*sorted(df.columns, reverse=is_reversed))\n    if not sort_nested:\n        return top_level_sorted_df\n\n    is_nested: bool = any(\n        isinstance(i.dataType, (StructType, ArrayType))\n        for i in top_level_sorted_df.schema\n    )\n\n    if not is_nested:\n        return top_level_sorted_df\n\n    fully_sorted_schema = sort_nested_cols(top_level_sorted_df.schema, is_reversed)\n    output = df.selectExpr(fully_sorted_schema)\n    result_dict = {}\n    for field in df.schema:\n        get_original_nullability(field, result_dict)\n\n    for field in output.schema:\n        fix_nullability(field, result_dict)\n\n    if not hasattr(SparkSession, \"getActiveSession\"):  # spark 2.4\n        spark = SparkSession.builder.getOrCreate()\n    else:\n        spark = SparkSession.getActiveSession()\n        spark = spark if spark is not None else SparkSession.builder.getOrCreate()\n\n    return output\n</code></pre>"},{"location":"reference/quinn/transformations/#quinn.transformations.to_snake_case","title":"<code>to_snake_case(s)</code>","text":"<p>Takes a string and converts it to snake case format.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to be converted.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string in snake case format.</p> Source code in <code>quinn/transformations.py</code> <pre><code>def to_snake_case(s: str) -&gt; str:\n\"\"\"Takes a string and converts it to snake case format.\n\n    :param s: The string to be converted.\n    :type s: str\n    :return: The string in snake case format.\n    :rtype: str\n    \"\"\"\n    return s.lower().replace(\" \", \"_\")\n</code></pre>"},{"location":"reference/quinn/transformations/#quinn.transformations.with_columns_renamed","title":"<code>with_columns_renamed(fun)</code>","text":"<p>Function designed to rename the columns of a <code>Spark DataFrame</code>.</p> <p>It takes a <code>Callable[[str], str]</code> object as an argument (<code>fun</code>) and returns a <code>Callable[[DataFrame], DataFrame]</code> object.</p> <p>When <code>_()</code> is called on a <code>DataFrame</code>, it creates a list of column names, applying the argument <code>fun()</code> to each of them, and returning a new <code>DataFrame</code> with the new column names.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable[[str], str]</code> <p>Renaming function</p> required <p>Returns:</p> Type Description <code>Callable[[DataFrame], DataFrame]</code> <p>Function which takes DataFrame as parameter.</p> Source code in <code>quinn/transformations.py</code> <pre><code>def with_columns_renamed(fun: Callable[[str], str]) -&gt; Callable[[DataFrame], DataFrame]:\n\"\"\"Function designed to rename the columns of a `Spark DataFrame`.\n\n    It takes a `Callable[[str], str]` object as an argument (``fun``) and returns a\n    `Callable[[DataFrame], DataFrame]` object.\n\n    When `_()` is called on a `DataFrame`, it creates a list of column names,\n    applying the argument `fun()` to each of them, and returning a new `DataFrame`\n    with the new column names.\n\n    :param fun: Renaming function\n    :returns: Function which takes DataFrame as parameter.\n    \"\"\"\n\n    def _(df: DataFrame) -&gt; DataFrame:\n        cols = [F.col(f\"`{col_name}`\").alias(fun(col_name)) for col_name in df.columns]\n        return df.select(*cols)\n\n    return _\n</code></pre>"},{"location":"reference/quinn/transformations/#quinn.transformations.with_some_columns_renamed","title":"<code>with_some_columns_renamed(fun, change_col_name)</code>","text":"<p>Function that takes a <code>Callable[[str], str]</code> and a <code>Callable[[str], str]</code> and returns a <code>Callable[[DataFrame], DataFrame]</code>.</p> <p>Which in turn takes a <code>DataFrame</code> and returns a <code>DataFrame</code> with some of its columns renamed.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable[[str], str]</code> <p>A function that takes a column name as a string and returns a new name as a string.</p> required <code>change_col_name</code> <code>Callable[[str], str]</code> <p>A function that takes a column name as a string and returns a boolean.</p> required <p>Returns:</p> Type Description <code>`Callable[[DataFrame], DataFrame]`</code> <p>A <code>Callable[[DataFrame], DataFrame]</code>, which takes a <code>DataFrame</code> and returns a <code>DataFrame</code> with some of its columns renamed.</p> Source code in <code>quinn/transformations.py</code> <pre><code>def with_some_columns_renamed(\n    fun: Callable[[str], str],\n    change_col_name: Callable[[str], str],\n) -&gt; Callable[[DataFrame], DataFrame]:\n\"\"\"Function that takes a `Callable[[str], str]` and a `Callable[[str], str]` and returns a `Callable[[DataFrame], DataFrame]`.\n\n    Which in turn takes a `DataFrame` and returns a `DataFrame` with some of its columns renamed.\n\n    :param fun: A function that takes a column name as a string and returns a\n    new name as a string.\n    :type fun: `Callable[[str], str]`\n    :param change_col_name: A function that takes a column name as a string and\n    returns a boolean.\n    :type change_col_name: `Callable[[str], str]`\n    :return: A `Callable[[DataFrame], DataFrame]`, which takes a\n    `DataFrame` and returns a `DataFrame` with some of its columns renamed.\n    :rtype: `Callable[[DataFrame], DataFrame]`\n    \"\"\"\n\n    def _(df: DataFrame) -&gt; DataFrame:\n        cols = [\n            F.col(f\"`{col_name}`\").alias(fun(col_name))\n            if change_col_name(col_name)\n            else F.col(f\"`{col_name}`\")\n            for col_name in df.columns\n        ]\n        return df.select(*cols)\n\n    return _\n</code></pre>"},{"location":"reference/quinn/extensions/","title":"Index","text":"<p>Extensions API.</p>"},{"location":"reference/quinn/extensions/dataframe_ext/","title":"Dataframe ext","text":""},{"location":"reference/quinn/extensions/spark_session_ext/","title":"Spark session ext","text":""},{"location":"reference/quinn/extensions/spark_session_ext/#quinn.extensions.spark_session_ext.create_df","title":"<code>create_df(spark, rows_data, col_specs)</code>","text":"<p>Creates a new DataFrame from the given data and column specs.</p> <p>The returned DataFrame is created using the StructType and StructField classes provided by PySpark.</p> <p>Parameters:</p> Name Type Description Default <code>rows_data</code> <code>list[tuple]</code> <p>the data used to create the DataFrame</p> required <code>col_specs</code> <code>list[tuple]</code> <p>list of tuples containing the name and type of the field</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>a new DataFrame</p> Source code in <code>quinn/extensions/spark_session_ext.py</code> <pre><code>def create_df(\n    spark: SparkSession,\n    rows_data: list[tuple],\n    col_specs: list[tuple],\n) -&gt; DataFrame:\n\"\"\"Creates a new DataFrame from the given data and column specs.\n\n    The returned DataFrame is created using the StructType and StructField classes provided by PySpark.\n\n    :param rows_data: the data used to create the DataFrame\n    :type rows_data: array-like\n    :param col_specs: list of tuples containing the name and type of the field\n    :type col_specs: list of tuples\n    :return: a new DataFrame\n    :rtype: DataFrame\n    \"\"\"\n    warnings.warn(\n        \"Extensions may be removed in the future versions of quinn. Please use `quinn.create_df()` instead\",\n        category=DeprecationWarning,\n        stacklevel=2,\n    )\n\n    struct_fields = [StructField(*x) for x in col_specs]\n    return spark.createDataFrame(data=rows_data, schema=StructType(struct_fields))\n</code></pre>"}]}